\documentclass[11pt]{article}

\author{Jacob Thomas Errington}
\title{Assignment \#4\\Discrete structures 2-- MATH 340}
\date{22 March 2017}

\usepackage[geometry,questions]{jakemath}
\usepackage{cancel}

\begin{document}

\maketitle

\question{Birthday paradox}

We are interested in the event ``there exists a pair of people with the same
birthday''.
To make things simpler, we look at the complement ``no two people have the same
birthday''.
Then, we cook up the following process, by numbering the $n$ people
$\range{n}$.
%
\begin{enumerate}
    \item
        Look at person one. Any birthday will do since nobody else has been
        chosen yet, so the probability of not having a common birthday with a
        previously chosen person is $\frac{d}{d}$ where $d$ is the number of
        days in the year.
        %
    \item
        Look at person two. The probability that they do not share a birthday
        with person one is $\frac{d-1}{d}$.
        %
    \item Look at person $k$. The probability that they do not share a birthday
        with any of the $k-1$ previous people is $\frac{d-(k-1)}{d}$.
\end{enumerate}
%
The probabiity that all these events occur at once is given by the product, so
we have that the probability that no two people out of $n$ share a birthday
among $d$ days is
%
\begin{equation*}
    p
    = \overbrace{
        \frac{d}{d} \times
        \frac{d-1}{d} \times
        \cdots \times
        \frac{d-n+1}{d}
    }^{\text{$n$ factors in total}}
    = \parens{1 - \frac{1}{d}}
        \times \cdots
        \times \parens{1 - \frac{1}{n-1}}
\end{equation*}
%
We take a logarithm on both sides to turn the product into a sum.
%
\begin{equation*}
    \ln p
    = \sum_{k=1}^{n-1} {
        \ln{\parens{1 - \frac{k}{d}}}
    }
    = \cdots
\end{equation*}
%
and use the fact that $\ln{1 + x} \approx x$ for small values of $x$ to
eliminate the logarithms on the right-hand side
%
\begin{equation*}
    \cdots
    = \sum_{k=1}^{n-1} {
        \frac{-k}{d}
    }
    = \frac{-1}{d} \sum_{k=1}^{n-1} k
    = \frac{-n(n-1)}{2d}
\end{equation*}
%
This gives a quadratic equation for $n$,
%
\begin{equation*}
    0 = \frac{1}{2d} n^2 + \frac{1}{2d} n - \ln p
\end{equation*}
%
Solving for $n$, we get
%
\begin{equation*}
    n
    = \frac{
        \frac{1}{2d}
        \pm
        \sqrt{
            \frac{1}{4d^2} - 4 \cdot \frac{1}{2d} \cdot \ln p
        }
    }{
        2 \cdot \frac{1}{2d}
    }
\end{equation*}
%
We are given that $d = 365$ and $1 - p = \frac{999}{1000}$, so we substitute to
find that
%
\begin{equation*}
    n = 71.54
\end{equation*}
%
Computing the probability backwards from $n = 71$, we find that the probability
that a collision occurs is indeed above $\frac{999}{1000}$. However, we also
tried $70$ and $69$, and found that $70$ is the least $n$ that gives a
sufficient probability.

\question{Quicksort}

Suppose $(x_1,\ldots,x_n)$ is a permuatation of $\range{n}$ chosen uniformly at
random.

\begin{prop}
    The probability that numbers $i$ and $j$ are compared,
    where $1 \leq j \leq j \leq n$,
    is
    \begin{equation*}
        \P{\text{$i$ is compared with $j$}} = \frac{2}{i - j + 1}
    \end{equation*}
\end{prop}

\begin{proof}
    There are two equivalent ways of thinking about this.
    The first is that we have a shuffled list, and pick the leftmost element as
    the pivot always.
    The second is that the list is already sorted, and we pick a random element
    as the pivot.
    This second interpretation is more convenient for our analysis.

    The numbers $i$ and $j$ will be compared only when they appear in the same
    sublist and one or the other is chosen as a pivot.
    Therefore if a pivot between $i$ and $j$ is ever chosen, then $i$ and $j$
    can never be compared.
    Notice that if $k$ is chosen as a pivot such that $k < i$ or $k > j$,
    then $i$ and $j$ are binned into the same group; the sublist $\range[i]{j}$
    is unaffected.

    Hence we arrive at our intuition for this problem: we think of it as a
    game. We pick an item at random in the list. If it is between $i$ and $j$,
    then we lose. If we pick $i$ or $j$, then we win. If we pick a number
    strictly below $i$ or strictly above $j$, then we get to pick again in the
    sublist made by dropping the elements up to $k$ or from $k$ onwards,
    respectively.

    The crux of this game is that the rule by which more choices are made does
    not affect the probability of winning or losing. Hence, we can assume
    without loss of generality that the game ends in the current turn.
    This means that we are looking at the sublist $\range[i]{j}$.
    But in this sublist, the probability of winning is exactly
    $\frac{2}{j - i + 1}$,
    as required.
\end{proof}

\begin{prop}
    The expected number of comparisons in randomized quicksort is
    %
    \begin{equation*}
        \E{X} = 2 \sum_{k=1}^{n-1} { \frac{n-k}{k+1} }
    \end{equation*}
\end{prop}

\begin{proof}
    Let $X_{ij}$ be an indicator random variable for the event
    ``$i$ is compared with $j$''.
    Hence, $\E{X_{ij}} = \P{X_{ij}} = \frac{2}{i - j + 1}$.
    By linearity of expectation, we find
    \begin{equation*}
        \E{X} = \sum_{i=1}^n \sum_{j=1}^n \frac{2}{i - j + 1}
    \end{equation*}

    To see how this sum can be simplified, it is illuminating to look at the
    following table for $n = 5$.

    \begin{center}
        \begin{tabular}{c | c c c c c}
            $j\backslash i$ & $1$ & $2$ & $3$ & $4$ & $5$ \\ \hline
            $1$ & & & & & \\
            $2$ & $2/2$ & & & & \\
            $3$ & $2/3$ & $2/2$ & & & \\
            $4$ & $2/4$ & $2/3$ & $2/2$ & & \\
            $5$ & $2/5$ & $2/4$ & $2/3$ & $2/2$ & \\
        \end{tabular}
    \end{center}

    By grouping the diagonals, we find
    %
    \begin{equation*}
        \E{X}
        = \sum_{k=2}^n {
            (n-k+1) \cdot \frac{2}{k}
        }
        = 2 \sum_{k=1}^n {
            \frac{n-k}{k+1}
        }
    \end{equation*}
    %
    as required.
\end{proof}

\question{Balls and bins 1}

\begin{prop}
    Suppose $n^{\frac{3}{2}}$ balls are dropped into $n$ bins uniformly at
    random.
    Let $M^*$ be the random variable representing the maximum number of balls
    in a bin.
    An upper bound on the expectation of this variabe is
    \begin{equation*}
        \E{M^*} = 1 + 3 \sqrt{\frac{\ln n}{\sqrt n}} + n
    \end{equation*}
\end{prop}

\begin{proof}
    Let $X_i$ be a random variable counting the number of balls in bin $i$.
    Note that each $X_i$ is binomially distributed:
    a trial is the action of dropping a ball, and its ``success'' probability
    is $p = \frac{1}{n}$.
    %
    Then the expected number of balls in any bin is
    \begin{equation*}
        \E{X_i} = n^{\frac{3}{2}} \frac{1}{n} = \sqrt{n}
    \end{equation*}
    %
    We will apply the nice form of the Chernoff bound, with
    $\delta = 3 \sqrt{\frac{\ln n}{\sqrt n}}$.
    Let $r^* = 1 + \delta$.
    %
    \begin{equation*}
        \P{X_i \geq r^* \sqrt{n}}
        \leq e^{
            \frac{-1}{3} \cancel{\sqrt{n}} 3^2 \frac{\ln n}{\cancel{\sqrt n}}
        }
        = \frac{1}{n^3}
    \end{equation*}
    %
    Then to compute a bound on the probability that the maximum $M^*$ is not
    less than $r^*$ via the union bound.
    %
    \begin{equation*}
        \P{M^* \geq r^*}
        = \P{\Union_i X_i \geq r^*}
        \leq \sum_{i=1}^n \cancelto{\frac{1}{n^3}}{\P{X_i \geq r^*}}
        = n \frac{1}{n^3} = \frac{1}{n^2}
    \end{equation*}
    %
    Then we can use this information to compute a bound on the expectation of
    $M^*$.
    %
    \begin{align*}
        \E{M^*}
        &= \sum_{k=0}^n {k \P{M^* = k}} \\
        &= \sum_{k=0}^{r^*} {
            \cancelto{\leq r^*}{k} \P{M^* = k}
        }
        + \sum_{k=r^*}^{n} {
            \cancelto{\leq n}{k} \P{M^* = k}
        } \\
        &\leq
        r^* \cancelto{\leq 1}{
            \sum_{k=0}^{r^*} \P{M^* = k}
        }
        + n \sum_{k=r^*}^n { \P{M^* = k } } \\
        &\leq r^* + n \frac{1}{n^2} \\
        &= 1 + 3 \sqrt{\frac{\ln n}{\sqrt n}} + n
    \end{align*}
    %
    which is better than the trivial upper bound, for large values of $n$
    (greater than $5$ or $6$).
\end{proof}

\question{Balls and bins 2}

Let there be $n$ different colors and $n$ balls of each color, for a total of
$n^2$ balls. The balls are distributed among $n$ boxes by selecting a box
uniformly at random for each ball, and placing it in the box if the box does
not already contain a ball of that color; else, the ball is discarded.

\begin{prop}
    Let $A_{ij}$ denote the event that box $i$ contains a ball of color $j$.
    For arbitrary $i$ and $j$, we have
    %
    \begin{equation*}
        \P{A_{ij}} = 1 - \parens{1 - \frac{1}{n}}^n
    \end{equation*}
\end{prop}

\begin{proof}
    Take an arbitrary box $i$ and an arbitrary color $j$.
    Consider the complementary event, that box $i$ contains no ball of color
    $j$. This means that whenever a ball of color $j$ was picked up, it was a
    box other that box $i$ that was chosen randomly. The probability of
    choosing a box other than box $i$ is $\frac{n - 1}{n} = 1 - \frac{1}{n}$.
    Hence, the probability that box $i$ contains \emph{no} ball of color $j$ is
    %
    \begin{equation*}
        \P{\neg A_{ij}} = \parens{1 - \frac{1}{n}}^n
    \end{equation*}
    %
    as there are $n$ balls of color $j$.
    Therefore the probability that box $i$ contains a ball of color $j$ is
    %
    \begin{equation*}
        \P{A_{ij}} = 1 - \parens{1 - \frac{1}{n}}^n
    \end{equation*}
\end{proof}

\begin{prop}
    Let $X$ be the number of balls that are thrown away.
    Then,
    %
    \begin{equation*}
        \E{X} = n^2 \parens{1 - \frac{1}{n}}^n
    \end{equation*}
\end{prop}

\begin{proof}
    Look at an arbitrary box $i$. We know that the probability of a ball of
    some color $j$ being in this box is
    $\P{A_{ij}} = 1 - \parens{1 - \frac{1}{n}}^n$.
    %
    If we think of each of ``being in the box'' for a certain color as a
    Bernoulli trial, then the number of balls in a given box is binomially
    distributed, with probability $p = \P{A_{ij}}$ and count $n$.
    %
    Denote by $B_i$ the number of balls in box $i$, so
    %
    \begin{equation*}
        \E{B_i} = n \parens{1 - \parens{1 - \frac{1}{n}}^n}
    \end{equation*}
    %
    But then the expected number of balls across all bins, by the linearity of
    expectation, is
    %
    \begin{equation*}
        \E{B}
        = \sum_i B_i
        = n^2 \parens{1 - \parens{1 - \frac{1}{n}}^n}
    \end{equation*}
    %
    So the expected number of balls that are discarded is the complement.
    %
    \begin{equation*}
        \E{X}
        = n^2 \parens{1 - \parens{1 - \parens{1 - \frac{1}{n}}^n}}
        = n^2 \parens{1 - \frac{1}{n}}^n
    \end{equation*}
\end{proof}

\begin{prop}
    With high probability, no box contains more than

    \begin{equation*}
        b = n \parens{ 1 - \parens{ 1 - \frac{1}{n} }^n } + 2 \sqrt{ n \ln n }
    \end{equation*}
\end{prop}

\begin{proof}
    We apply the Chernoff bound for some box, to see an upper bound on the
    probability that the number of balls in that box exceeds the desired bound.
    In that case, we have $\mu = n \parens{1 - \parens{1 - \frac{1}{n}}^n}$
    and $\delta = 2 \sqrt{n \ln n}$.
    %
    \begin{align*}
        \P{B_i \geq (1 + \delta) \mu} &\leq e^{-\frac{1}{3} \mu \delta^2} \\
        \P{B_i \geq n \parens{1 - \parens{1 - \frac{1}{n}}^n} + 2 \sqrt{n \ln n}}
        &\leq e^{
            -\frac{1}{3}
            n \parens{1 - \parens{1 - \frac{1}{n}}^n}
            4 n \ln n
        }
        = n^{
            -\frac{4}{3} n^2 \parens{1 - \parens{1 - \frac{1}{n}}^n}
        }
    \end{align*}
    %
    This is a fairly small quantity, even for modest values of $n$.
    For instance, when $n=4$ we are at a probability on the order of $10^{-9}$.

    However, this is an upper bound on the probability that given box exceeds
    our bound $b$.
    %
    We use the union bound to find an upper bound on the probability that
    \emph{any} box exceeds $b$ in ball count. This merely adds a factor of $n$,
    which is negligeable.
    %
    Then, we can translate this upper bound on the probability that any box
    exceeds $b$ into a lower bound on the probability that no box exceeds $b$
    by taking the complement.
    Let $A$ denote the event that no box exceeds $b$ in ball count.
    The bound we find on the probability of $A$ is
    \begin{equation*}
        \P{A} \geq 1 - n^{
            1 - \frac{4}{3} n^2 \parens{1 - \parens{1 - \frac{1}{n}}^n}
        }
    \end{equation*}
\end{proof}

\question{Chernoff bound}

\begin{prop}
    The following variant of the Chernoff bound holds.

    \begin{equation*}
        \P{X \leq (1 - \delta)\mu}
        \leq
        \parens{
            \frac{
                e^{-\delta}
            }{
                (1 - \delta)^{1 - \delta}
            }
        }^\mu
        \leq
        e^{-\frac{\mu \delta^2}{2}}
    \end{equation*}
\end{prop}

\begin{proof}
    To see the first inequality, we make a symmetry argument.
    Consider the variables $X^\prime_i$ obtained by reflecting $X_i$ across
    $1/2$.
    %
    Then we have a new expectation $\mu^\prime$ for these variables, which
    coincides with what we get if we reflect $\mu$ across $1/2$.
    %
    It is clear that the probability of deviation \emph{above} the mean by
    $\delta$ in this new setting is the same as the probability of deviation
    \emph{below} the mean by $\delta$ in the old setting.

    Next, for the second inequality, observe, the following sequence of logical
    equivalences.
    %
    \begin{align*}
        \parens{
            \frac{e^{-\delta}}{(1-\delta)^{1-\delta}}
        }^{\cancel{\mu}}
        &\leq
        e^{- \frac{\cancel{\mu} \delta^2}{2}}
        %
        \\
        \ln{\frac{e^{-\delta}}{(1-\delta)^{1-\delta}}}
        &\leq
        -\frac{\delta^2}{2}
        %
        \\
        -\delta - (1-\delta) \ln{\parens{1-\delta}}
        &\leq
        -\frac{\delta^2}{2}
        %
        \\
        \ln{\parens{1 - \delta}}
        &\leq
        \frac{\frac{\delta^2}{2} - \delta}{1 - \delta}
        %
        \\
        - \delta - \frac{1}{2} \delta^2 - \frac{1}{3} \delta^3 - \cdots
        &\leq
        \frac{\delta(\delta - 2)}{2(1 - \delta)}
        \leq - \delta
    \end{align*}
    %
    In particular, we use the Taylor expansion of the logarithm, and note that
    $\frac{\delta - 2}{2} \leq \delta - 1$, since $0 < \delta < 1$ by
    assumption.
\end{proof}

\end{document}
