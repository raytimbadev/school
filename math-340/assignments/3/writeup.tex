\documentclass[letterpaper,11pt]{article}

\author{Jacob Thomas Errington}
\title{Assignment \#3\\Discrete structures 2 -- MATH 340}
\date{8 March 2017}

\usepackage[margin=2.0cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}

\newtheorem{prop}{Proposition}
\DeclareMathOperator{\Prob}{P}
\renewcommand{\P}[1]{\Prob{\parens{#1}}}
\DeclareMathOperator{\prob}{p}
\newcommand{\p}[2][]{%
    \def\temp{#2}
    \ifx\temp\empty
        \prob{\parens{#2}}
    \else
        \prob_{#1}{\parens{#2}}
    \fi
}
\DeclareMathOperator{\Expect}{\mathbb{E}}
\newcommand{\E}[1]{\Expect{\left[#1\right]}}
\DeclareMathOperator{\Var}{\mathbb{V}}
\newcommand{\V}[1]{\Var{\parens{#1}}}
\DeclareMathOperator{\BinOp}{Bin}
\newcommand{\Bin}[1]{\BinOp{\parens{#1}}}

\newcommand{\R}{\mathbb{R}}

\newcommand{\parens}[1]{\left(#1\right)}
\newcommand{\Union}{\bigcup}
\newcommand{\union}{\cup}
\newcommand{\Intersn}{\bigcap}
\newcommand{\intersn}{\cap}
\newcommand{\symdiff}{\,\Delta\,}
\newcommand{\fact}{!\,}
\newcommand{\given}{\;\vert\;}
\newcommand{\compl}{^c}
\newcommand{\inv}{^{-1}}
\newcommand{\compose}{\circ}

\renewcommand{\thesection}{Question \arabic{section}}
\newcommand{\question}{\section}

\newcommand{\dd}[1]{\frac{\mathrm{d}}{\operatorname{d} #1}}

\begin{document}

\maketitle

\question{Bayes's Theorem}

\begin{enumerate}
    \item
        This problem can be solved straighforwardly without Bayes's theorem
        by enumerating the sample space, $S = \{GG, BG, GB, BB\}$, where $BG$
        for instance denotes the outcome that the first child is male and the
        second is female. As we know that one child is male, this restricts the
        sample space to $S^\prime = \{BG, GB, BB\}$. In this space, the event
        $E = \text{``having one boy and one girl''}$ is true for both the $BG$
        and $GB$ outcomes.
        Hence, $\P{E} = \frac{2}{3}$.

    \item
        Let $Y$ denote the number of heads and $X$ denote the face value of the
        die. We wish to find $\P{X = 6 \given X = Y}$, so we apply Bayes's
        theorem.
        \begin{equation}
            \label{eq:bleh}
            \P{X = 6 \given X = Y} = \frac{
                \P{Y = X \given X = 6} \P{Y = X}
            }{
                \P{X = 6}
            }
        \end{equation}

        First, we know that $\P{Y = X \given X = 6}$ is simply the probability
        of getting heads six times $\P{Y = 6} = \frac{1}{2^6}$.

        Second, $\P{X = 6} = \frac{1}{6}$ is the probability of rolling a six.

        Finally, we find $\P{Y = X}$ by using the Law of Total Probability, as
        the different values of $X$ partition the space.
        \begin{equation*}
            \P{Y = X} = \sum_{i = 1}^6 {
                \P{Y = X \given X = i} \P{X = i}
            }
            =
            \sum_{i=1}^6 {
                \frac{1}{2^i} \frac{1}{6}
            }
            = \frac{21}{128}
        \end{equation*}

        Substituting into \eqref{eq:bleh} gives
        \begin{equation*}
            \P{X = 6 \given X = Y}
            = \frac{63}{4096} \sim 0.0154
        \end{equation*}

    \item
        Let $T$ be the event ``the new employee tests positive''. Then $\neg T$
        denotes the event that the employee tests negative.
        Let $D$ be the event ``the new employee uses the drug''.
        We know that $\P{T \given \neg D} = 0.02$
        and $\P{\neg T \given D} = 0.1$.
        We wish to find $\P{D \given T}$, the probability that the employee
        uses the drug given that they test positive.
        Applying Bayes's theorem gives
        \begin{equation*}
            \label{eq:employee-bayes}
            \P{D \given T} = \frac{
                \P{T \given D} \P{D}
            }{
                \P{T}
            }
        \end{equation*}

        Since the probability of testing negative when using the drug is $0.1$,
        the complement is the probability of testing positive when using the
        drug, i.e. $\P{T \given D} = 1 - \P{\neg T \given D} = 0.9$.

        All that remains to find is $\P{T}$. $\{D, \neg D\}$ partitions the
        space, so we apply the Law of Total Probability to obtain
        \begin{equation*}
            \P{T} = \P{T \given D} \P{D} + \P{T \given \neg D} \P{\neg D}
            = 0.9 \times 0.01 + 0.02 \times 0.99
            = 0.0288
        \end{equation*}

        Substituting these quantities into \eqref{eq:employee-bayes} gives
        \begin{equation*}
            \P{D \given T} = \frac{
                0.9 \times 0.01
            }{
                0.0288
            }
            = 0.3125
        \end{equation*}
\end{enumerate}

\question{Monty Hall variant}

Suppose without loss of generality that we picked door $A$ and that Monty
picked door $B$. Then the prize is behind either door $A$ or door $C$. Let
Let $A$, $B$, $C$ denote the events ``the prize is behind door $A$, $B$, $C$''
respectively. Let $M_i$ be the event that Monty selects door $i$.

We would like to compare the probability that the prize is behind door $A$
given that Monty chooses door $B$ \emph{and} that door $B$ does not contain the
prize $\P{A \given M_B \land \neg B}$
with the probability that the prize is behind door $C$ given that Monty chooses
door $B$ \emph{and} that door does  $\P{C \given M_B}$.

Applying Bayes's theorem, we have
\begin{equation}
    \label{eq:monty-bayes}
    \P{A \given M_B} = \frac{
        \P{M_B \given A} \P{A}
    }{
        \P{M_B}
    }
\end{equation}

The probability that Monty selects door $B$ given that the prize is behind door
$A$ is simply a half, since he selects from doors $A$ and $B$ with equal
probability. To find the probability that Monty selects door $B$ under no
assumptions, we use the Law of Total Probability.

\begin{equation*}
    \P{M_B}
    = \P{M_B \given A} \P{A}
    + \P{M_B \given B} \P{B}
    + \P{M_B \given C} \P{C}
    = \frac{1}{2} \times \frac{1}{3}
    + 0 \times \frac{1}{3}
    + \frac{1}{2} \times \frac{1}{3}
    = \frac{1}{3}
\end{equation*}

Substituting into \eqref{eq:monty-bayes} gives
\begin{equation*}
    \P{A \given M_B} = \frac{
        \frac{1}{2} \times \frac{1}{3}
    }{
        \frac{1}{3}
    }
    = \frac{1}{2}
\end{equation*}

By symmetry we have the same probability $\P{C \given M_B} = \frac{1}{2}$.

Hence, it does not matter whether we switch doors.

\question{Linearity of expectation}

Denote by $W_i$ the expected winnings of roll $i$.
Then the overall winnings from playing the game is $\sum_{i=1}^n W_i$.
Note that on the first roll, the winnings are zero because there is no previous
roll, so the overall winnings can be expressed as $\sum_{i=2}^n W_i$.

We would like to find $\P{W_i = 1}$ when $i > 1$.
Semantically, this probability is the same as the probability of the previous
roll's face value differing by one from the current roll's face value.
Denote by $R_i$ the face value of roll $i$. Then,
\begin{equation*}
    \P{R_i - R_{i-1} = 1 \lor R_{i-1} - R_i = 1}
    = \P{R_i - R_{i-1} = 1} + \P{R_{i-1} - R_i = 1}
\end{equation*}
as these events cannot happen at the same time.

For each of $\P{R_i - R_{i-1} = 1}$ and $\P{R_{i-1} - R_i = 1}$, there are $36$
outcomes overall (different values for $(R_i, R_{i-1})$ but only $10$ that
produce a difference of one, so the probability for each of these is
$\frac{10}{36}$.
Hence, the probability of winning $ 1\$ $ is $\frac{20}{36}$.

Next, we would like to find $\P{W_i = -1}$ when $i > 1$.
Again by the semantics, this is the same as the probability of rolling the same
value twice, i.e. $\P{R_i = R_{i-1}}$. We can partition the space on the value
of the previous roll and apply the Law of Total Probability.
\begin{equation*}
    \P{R_i = R_{i-1}}
    = \sum_{x = 1}^6 {
        \P{R_i = R_{i-1} \given R_{i-1} = x} \P{R_{i-1} = x}
    }
    = \frac{1}{6} \times \frac{1}{6}
    = \frac{1}{36}
\end{equation*}

Now we can find the expected winnings of roll $i$.
\begin{equation*}
    \E{W_i}
    = \frac{1}{36} \times (-1)
    + \frac{20}{36} \times 1
    = \frac{19}{36}
    \sim 0.53
\end{equation*}

Now we can use the linearity of expectation to compute the winnings of $n$
rolls,
\begin{equation*}
    \E{\sum_{i=2}^n W_i} = \sum_{i=2}^n \E{W_i} = (n-1)\frac{19}{36}
\end{equation*}

Note that the sum begins at two in order to discount the first roll, which
produces no winnings.

\question{Markov inequality}

\begin{prop}
    Let $X$ be a random variable taking only non-negative values and let
    $c \in \R$ be a positive constant.
\end{prop}

\begin{proof}
    Let $I_E$ denote an indicator random variable for an event $E$, taking the
    value $1$ when $E$ occurs and $0$ otherwise.
    Hence, if $X < c$, then $I_{X \geq c} = 0$;
    if $X \geq c$, then $I_{X \geq c} = 1$.
    Therefore,
    \begin{equation}
        \label{eq:indicate-c}
        c I_{X \geq c} \leq X
    \end{equation}

    In general, $A < B \implies \E{A} < \E{B}$, i.e. expectation is monotonic.
    Taking expectation on both sides in \eqref{eq:indicate-c} gives
    \begin{equation}
        \label{eq:expect-c}
        \E{c I_{X \geq c}} \leq \E{X}
    \end{equation}
    We use the linearity of expectation to change the left-hand side from an
    expectation to a probability.
    \begin{equation*}
        c \E{I_{X \geq c}}
        = c \parens{
            1 \times \P{X \geq c}
            + 
            0 \times \P{X < c}
        }
        = c \P{X \geq c}
    \end{equation*}

    Substituting this into \eqref{eq:expect-c} and dividing across by $c$,
    since it is positive, gives the desired statement.
\end{proof}

\question{Binomial distribution}

The Stanley Cup winner is determined in the final series between two teams. The
first team to win four games wins the Cup. Suppose that the Montreal Canadiens
advance to the final series, have a probability $0.6$ of winning each game, and
that the game results are independent of each other.

We wish to find the probability that the canadiens win the Stanley Cup. First,
we will see how many games need to be played to determine the winner of the
Cup.

\begin{prop}
    Seven games are required to determine the winner.
\end{prop}

\begin{proof}
    When a team wins a game, the other team loses a game. Hence when team $A$
    has won $n$ games out of $m$, the opposing team $B$ must have won $m - n$
    games. We want to find a value of $m$ so that either $m - n \geq 4$ or
    $n \geq 4$.
    First, to see that eight games is too many, just notice that if one team
    wins four games, the other team has also won four games, but this is a
    contradiction as then two teams would win the cup.
    Next, if fewer than seven games are played, then it is possible to
    distribute the wins so that neither team has won a total of four times,
    leaving the winner of the cup undetermined.
    Hence, exactly seven games are required to determine the winner.
\end{proof}

Hence, once seven games have been played, a winner is determined. No eighth
game ever need be played.

Winning the Cup means winning the first four games; or losing one of the first
four, but winning the fifth; or losing two of the first fifth, but winning the
sixth; or losing three of the first sixth, but winning the seventh. Recall that
by the proposition, if four of the (first) seven games are lost, then the other
team wins the Cup. This is a binomial distribution.

\begin{equation*}
    \P{\text{Canadiens win}}
    = p^4
    + {5 \choose 4} p^4 (1-p)
    + {6 \choose 4} p^4 (1-p)^2
    + {7 \choose 4} p^4 (1-p)^3
    = \frac{15471}{15625}
    = 0.990144
\end{equation*}

\question{Geometric distribution}

Suppose we run repeated independent Bernoulli trials with success probability
$p$ until we obtain a success. Let $X$ denote the number of trials needed
before we obtain a success.

We want to find $\P{X = k}$, i.e. the probability that $k$ trials are needed
before encountering the first successful one. This means that $k$ trials were
unsuccessful and that the $k+1$\textsuperscript{th} one is successful. Since
the trials are independent, we can simply multiply (or rather exponentiate) the
probabilities. Hence,
\begin{equation*}
    \P{X = k} = (1-p)^k p
\end{equation*}

\begin{prop}
    The expectation on the number of trials before one is successful is
    \begin{equation*}
        \E{X} = \frac{1}{p}
    \end{equation*}
\end{prop}

\begin{proof}
    The proof boils down to using the definition of expectation, and playing
    with the series.

    \begin{align*}
        \E{X}
        &= \sum_{k=0}^\infty {
            (1 - p)^k p k
        } \\
        &= p (1-p) \sum_{k=0}^\infty {
            (1-p)^{k-1} k
        } \\
        &= p (1-p) \dd{p} \parens{
            - \sum_{k=0}^\infty {
                (1-p)^k
            }
        } \\
        &= p (1-p) \dd{p}\parens{-\frac{1}{p}} \\
        &= \frac{1}{p}
    \end{align*}
\end{proof}

\end{document}
