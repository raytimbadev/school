\documentclass[11pt,letterpaper]{article}

\usepackage[margin=2.0cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}

\newtheorem{proposition}{Proposition}

\DeclareMathOperator{\vdeg}{deg}
\newcommand{\degb}[1]{\vdeg_b{#1}}
\newcommand{\Z}{\mathbb{Z}}

\author{Jacob Thomas Errington (260636023)}
\title{Assignment \#2\\Advanced theory of computation -- COMP 531}
\date{15 February 2015}

\begin{document}

\maketitle

\section{Log-space reductions}

\begin{proposition}
    If both $f$ and $g$ are log-space computable functions, then $f \circ g$ is
    also a log-space computable function.
\end{proposition}

\begin{proof}
    Since $f$ and $g$ are computable, we have Turing machines $M_f$ and $M_g$
    that implement them with the appropriate space complexities. We will
    construct a Turing machine $M$ of logarithmic space complexity implementing
    $f \circ g$.

    The na\"ive strategy for constructing $M$ is to compute $f(w)$ by
    simulating $M_f$, and then feed $f(w)$ as input to a simulation of $M_g$.
    This however does not work since the output of $M_f$ might have length
    polynomial in $w$! It suffices to recognize that we can compute select
    elements of the output of $M_f$ in an on-demand fashion.

    In $M$, we represent the position of the $M_g$ simulation's read head with
    a counter: when the simulation would move the read head to the right, the
    counter is decremented; when the simulation would move the read head to the
    left, the counter is decremented. When the simulation of $M_g$ performs a
    read, $M$ performs a simulation of $M_f$ but ``jams'' the simulated write
    head, so to speak. Consequently, letters that are written by the $M_f$
    simulation overwrite each other. Each time an $M_f$ write is simulated, a
    copy of the counter is decremented; this allows $M$ to keep track of how
    many letters have been written. When this counter reaches zero, then the
    letter under the write head of the $M_f$ simulation can be used as the
    letter under the read head of the $M_g$ simulation.

    The nonconstant extra space required by this procedure is due to the
    counters. There is one primary counter used to represent the read head of
    the $M_g$ simulation, and one copy used when a new letter needs to be
    computed. The size of this counter is logarithmically bounded by the size
    of $f(w)$, which is at most polynomial in $w$. Hence overall, the
    nonconstant size is logarithmically bounded in the size of $w$ as required.
\end{proof}

\section{Father-son cycle detection}

\begin{proposition}
    The father-son algorithm terminates in finite time.
\end{proposition}

\begin{proof}
    The algorithm's termination criteria are the following.

    \begin{itemize}
        \item
            For all $v \in V$, for all $(v, w) \in E$, if the son leaves along
            $(v, w)$, then he returns along $(w, v)$. (This is the success
            criterion.)

        \item
            For some $v \in V$, for some $(v, w) \in E$, if the son leaves
            along $(v, w)$, then he returns along some $e \neq (w, v)$. (This
            is the failure criterion.)
    \end{itemize}

    These critera are conceptually the same: one is the negation of the other.
    Indeed, the only way that either could fail to be met is by the son
    ``getting stuck'' after leaving.

    Suppose that the son gets stuck in a cycle $K = (u, v, \cdots, w, u)$. In
    particular, the son will travel through $(w, u, v)$ infinitely many times.
    Let's consider how the son entered this cycle: the son arrived at $u$ from
    some vertex $w^\prime$, and subsequently went to vertex $v$. This implies
    that if the edge $(u, v)$ is the $j$th edge at $v$, then $(w^\prime, u$ is
    the $(j-1)$th edge. The description of the cycle on the other hand implies
    that $w$ is also the $(j-1)$th edge at $v$. We deduce that $w = w^\prime$.
    The key conclusion though is that the son cannot \emph{enter} a cycle. He
    is begins on a vertex that is a part of a cycle or he will never get caught
    in a cycle.

    Now we can examine the behaviour of the son after leaving the starting
    vertex $u$ where the father is. Suppose that the son never returns to $u$.
    The son must thus have entered a loop. From the result in above, we know
    that the son cannot ``enter'' a loop; he must have started in one. Hence
    the vertex $u$ is a member of the looping set of vertices and the son will
    in fact return to the father.

    Finally, we can state that the algorithm terminates in finite time since
    the son always returns to the father and the father lets the son go only a
    finite number of times (each vertex has finite degree and there are
    finitely many vertices).
\end{proof}

\begin{proposition}
    The father-son algorithm is correct.
\end{proposition}

\begin{proof}
    First, we will show that if there does not exist a cycle in the graph, then
    the son will always return along the edge that he left along.

    Suppose not, i.e. that the son leaves along an edge $(v, w)$ but returns
    along $(u, v) \neq (v, w)$. The sequence of vertices visited by the son
    forms a cycle, which is a contradiction.

    Next, we will show that if there exists a cycle in the graph, then there
    exists a vertex and an edge such that if the son leaves along that edge
    then he will return along a different edge.

    Let $K = (u, v, \cdots, w, u)$ be a cycle in the graph and suppose the son
    first leaves from $u$ along $(u, v)$. If the son returns along $(w, u)$,
    then the cycle has been detected. In fact, if the son returns along
    $(w^\prime, u)$ for any $w^\prime \neq v$, then the cycle will be detected.

    That leaves the case of the son returning along $(v, u)$. Suppose the son
    returns along $(v, u)$. This means that the son had to have ``doubled
    back'' at some point while travelling along $K$. Additionally, this means
    that the son had to have departed from $K$ at some point. Let $s$ be the
    furthest vertex along $K$ upon arriving at which from a vertex in $K$, the
    son then goes on to some vertex not in $K$. Several cases arise.

    \begin{itemize}
        \item
            If the son returns to $s$ along the same edge as he departed
            (repeatedly), then he will eventually move on to another vertex in
            $K$. This is a contradiction since in doing so, the son will
            eventually return to $u$ along $(w, u)$, so this case cannot arise.

        \item
            If the son returns to $s$ along a different edge, then when the
            father is or was at $s$ later or ealier, respectively, in the
            algorithm, then due to the memorylessness of the cycle searching
            principle, this behaviour will occur then and the cycle will be
            detected.

        \item
            If the son does not return to $s$ but instead returns to some other
            vertex $t$ along $K$, then if that vertex is further along $K$ than
            $s$ then we can repeat this same argument; else, if that vertex is
            earlier along $K$ than $s$, then when the father is positioned at
            $t$ and has the son leave such that he will eventually reach $s$
            and return to $t$, the son will be doing so along a different edge
            than when he left, so this cycle will be detected.
    \end{itemize}
\end{proof}

\section{Implementing symmetric functions}

A symmetric function $f : \{0, 1\}^n \to \{0, 1\}$ can be thought of as a
function $f : n \to \{0, 1\}$ from the $n$-set, since the order of the inputs
doesn't matter. With that in mind, our goal is to construct a circuit that can
``figure out'' which $i \in n$ is represented by the input bits and output the
according $f(i)$.

First, we can use a $MAJ$ gate to implement a check that the number of input
bits set to $1$ \emph{at least} a given number $i$. We start by wiring all
input bits to the gate.

We now have to pad the gate's inputs with constants to get the right behaviour.
Our padding scheme is to add $i$ `$0$'s and $n - i + 1$ `$1$'. The total number
of input wires then comes to $2n + 1$ which is always odd, of which $n - i + 1$
are always on. The threshold for majority is between $n$ and $n+1$. Hence, at
least $i$ of the original $n$ variable input wires are set to $1$ if and only
if the proportion of input wires set to $1$ is at least $\frac{n + 1}{2n + 1}$,
which is just over half.

Second, we can use a $MAJ$ gate to implement a check that the number of input
bits set to $1$ is \emph{at most} a given number $i$. We begin as before by
wiring all the circuit input bits to the gate. This time however we wire all
the \emph{negated} variable inputs.

Once again, we will pad the gate's inputs with constants. The padding scheme is
different this time. We add $i + 1$ constant `$1$'s and $n - 1$ constant
`$0$'s.

Using two of these gates in unison, we can check for equality to a given $i$.
Now we can get into the meat of the construction of the circuit. For each $i
\in n$ such that $f(i) = 1$, we add a two $MAJ$ gates, one to implement a check
that the number of inputs is at most $i$ called $g_{\leq i}$ and the other to
implement a check that the number of inputs is at least $i$ called
$g_{\geq i}$. Call each such pair of gates $G_i$.

Next, we add one final $MAJ$ gate whose output is the output of the circuit. We
wire the outputs of all the gates constructed in the previous step to this
gate. Let's analyze this situation.

Let $I = \{i \in n | f(i) = 1\}$ and let $i$ be the sum of the input bits.
There are $2|I|$ wires now coming into this final gate.
If $i \in I$, then both of the gates in $G_i$ will output $i$. All other $G_j$
for $j \neq i$ will have only one member gate emitting $1$ (else we reach a
contradiction). In other words if $i \in I$, then the number of wires carrying
$1$ coming into the final gate is $|I| + 1$, which is a majority.
On the other hand, if $i \notin I$, then each gate pair $G_i$ for all $i \in n$
will have only one member gate outputting $1$. Hence, the sum of the inputs
coming into the final gate is $|I|$, which is not a majority.

This concludes the construction.

\end{document}
