\documentclass[11pt,letterpaper]{article}

\usepackage[margin=2.0cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}

\newtheorem{proposition}{Proposition}

\DeclareMathOperator{\vdeg}{deg}
\newcommand{\degb}[1]{\vdeg_b{#1}}
\newcommand{\Z}{\mathbb{Z}}

\begin{document}

\section{Log-space reductions}

\begin{proposition}
    If both $f$ and $g$ are log-space computable functions, then $f \circ g$ is
    also a log-space computable function.
\end{proposition}

\begin{proof}
    Since $f$ and $g$ are computable, we have Turing machines $M_f$ and $M_g$
    that implement them with the appropriate space complexities. We will
    construct a Turing machine $M$ of logarithmic space complexity implementing
    $f \circ g$.

    The na\"ive strategy for constructing $M$ is to compute $f(w)$ by
    simulating $M_f$, and then feed $f(w)$ as input to a simulation of $M_g$.
    This however does not work since the output of $M_f$ might have length
    polynomial in $w$! It suffices to recognize that we can compute select
    elements of the output of $M_f$ in an on-demand fashion.

    In $M$, we represent the position of the $M_g$ simulation's read head with
    a counter: when the simulation would move the read head to the right, the
    counter is decremented; when the simulation would move the read head to the
    left, the counter is decremented. When the simulation of $M_g$ performs a
    read, $M$ performs a simulation of $M_f$ but ``jams'' the simulated write
    head, so to speak. Consequently, letters that are written by the $M_f$
    simulation overwrite each other. Each time an $M_f$ write is simulated, a
    copy of the counter is decremented; this allows $M$ to keep track of how
    many letters have been written. When this counter reaches zero, then the
    letter under the write head of the $M_f$ simulation can be used as the
    letter under the read head of the $M_g$ simulation.

    The nonconstant extra space required by this procedure is due to the
    counters. There is one primary counter used to represent the read head of
    the $M_g$ simulation, and one copy used when a new letter needs to be
    computed. The size of this counter is logarithmically bounded by the size
    of $f(w)$, which is at most polynomial in $w$. Hence overall, the
    nonconstant size is logarithmically bounded in the size of $w$ as required.
\end{proof}

\section{Father-son cycle detection}

\begin{proposition}
    The father-son algorithm terminates in finite time.
\end{proposition}

\begin{proof}
    The algorithm's termination criteria are the following.

    \begin{itemize}
        \item
            For all $v \in V$, for all $(v, w) \in E$, if the son leaves along
            $(v, w)$, then he returns along $(w, v)$. (This is the success
            criterion.)

        \item
            For some $v \in V$, for some $(v, w) \in E$, if the son leaves
            along $(v, w)$, then he returns along some $e \neq (w, v)$. (This
            is the failure criterion.)
    \end{itemize}

    These critera are conceptually the same: one is the negation of the other.
    Indeed, the only way that either could fail to be met is by the son
    ``getting stuck'' after leaving.

    Suppose that the son gets stuck in a cycle $K = (u, v, \cdots, w, u)$. In
    particular, the son will travel through $(w, u, v)$ infinitely many times.
    Let's consider how the son entered this cycle: the son arrived at $u$ from
    some vertex $w^\prime$, and subsequently went to vertex $v$. This implies
    that if the edge $(u, v)$ is the $j$th edge at $v$, then $(w^\prime, u$ is
    the $(j-1)$th edge. The description of the cycle on the other hand implies
    that $w$ is also the $(j-1)$th edge at $v$. We deduce that $w = w^\prime$.
    The key conclusion though is that the son cannot \emph{enter} a cycle. He
    is begins on a vertex that is a part of a cycle or he will never get caught
    in a cycle.

    Now we can examine the behaviour of the son after leaving the starting
    vertex $u$ where the father is. Suppose that the son never returns to $u$.
    The son must thus have entered a loop. From the result in above, we know
    that the son cannot ``enter'' a loop; he must have started in one. Hence
    the vertex $u$ is a member of the looping set of vertices and the son will
    in fact return to the father.

    Finally, we can state that the algorithm terminates in finite time since
    the son always returns to the father and the father lets the son go only a
    finite number of times (each vertex has finite degree and there are
    finitely many vertices).
\end{proof}

\begin{proposition}
    The father-son algorithm is correct.
\end{proposition}

\begin{proof}
    First, we will show that if there does not exist a cycle in the graph, then
    the son will always return along the edge that he left along.

    Suppose not, i.e. that the son leaves along an edge $(v, w)$ but returns
    along $(u, v) \neq (v, w)$. The sequence of vertices visited by the son
    forms a cycle, which is a contradiction.

    Next, we will show that if there exists a cycle in the graph, then there
    exists a vertex and an edge such that if the son leaves along that edge
    then he will return along a different edge.

    Let $K = (u, v, \cdots, w, u)$ be a cycle of minimal size in the graph and
    suppose the son first leaves from $u$ along $(u, v)$. Now we will
    systematically rule out the possibility of the son returning to $u$ along
    $(v, u)$. Consider that in order to ``double back'' along an edge of rank
    $i$ after arriving at a vertex $u$, according to the description of the
    algorithm, one of the following circumstances must arise.

    \begin{itemize}
        \item
            The vertex $u$ that the son arrives at by following the edge has
            degree one.

        \item
            The son leaves from $u$ along an edge of rank $i+1$ and eventually
            returns along an edge of rank $i-1$.
    \end{itemize}

    In the first situation, we reach a contradiction of the existence of a
    cycle: each vertex in a cycle must have degree at least two.

    In the second situation, we reach a contradiction of the minimality of the
    cycle: a smaller cycle could be constructed by considering the sequence of
    vertices visited by the son when leaving along the edge of rank $i+1$ until
    he returns to the vertex $u$.

    Since the father will make the son leave from every vertex and along every
    adjacent edge thereof, the son will in fact eventually wind up in the
    situation where he is travelling along a cycle of minimal size.
    Consequently he will return to the starting vertex along a different edge
    and the algorithm will report the detection of a cycle, as required.
\end{proof}

\section{Implementing symmetric functions}

A symmetric function $f : \{0, 1\}^n \to \{0, 1\}$ can be thought of as a
function $f : n \to \{0, 1\}$ from the $n$-set, since the order of the inputs
doesn't matter. With that in mind, our goal is to construct a circuit that can
``figure out'' which $i \in n$ is represented by the input bits and output the
according $f(i)$.

First, we can use a $MAJ$ gate to implement a check that the number of input
bits set to $1$ \emph{at least} a given number $i$. We start by wiring all
input bits to the gate.

We now have to pad the gate's inputs with constants to get the right behaviour.
Our padding scheme is to add $i$ `$0$'s and $n - i + 1$ `$1$'. The total number
of input wires then comes to $2n + 1$ which is always odd, of which $n - i + 1$
are always on. The threshold for majority is between $n$ and $n+1$. Hence, at
least $i$ of the original $n$ variable input wires are set to $1$ if and only
if the proportion of input wires set to $1$ is at least $\frac{n + 1}{2n + 1}$,
which is just over half.

Second, we can use a $MAJ$ gate to implement a check that the number of input
bits set to $1$ is \emph{at most} a given number $i$. We begin as before by
wiring all the circuit input bits to the gate. This time however we wire all
the \emph{negated} variable inputs.

Once again, we will pad the gate's inputs with constants. The padding scheme is
different this time. We add $i + 1$ constant `$1$'s and $n - 1$ constant
`$0$'s.

Using two of these gates in unison, we can check for equality to a given $i$.
Now we can get into the meat of the construction of the circuit. For each $i
\in n$ such that $f(i) = 1$, we add a two $MAJ$ gates, one to implement a check
that the number of inputs is at most $i$ called $g_{\leq i}$ and the other to
implement a check that the number of inputs is at least $i$ called
$g_{\geq i}$. Call each such pair of gates $G_i$.

Next, we add one final $MAJ$ gate whose output is the output of the circuit. We
wire the outputs of all the gates constructed in the previous step to this
gate. Let's analyze this situation.

Let $I = \{i \in n | f(i) = 1\}$ and let $i$ be the sum of the input bits.
There are $2|I|$ wires now coming into this final gate.
If $i \in I$, then both of the gates in $G_i$ will output $i$. All other $G_j$
for $j \neq i$ will have only one member gate emitting $1$ (else we reach a
contradiction). In other words if $i \in I$, then the number of wires carrying
$1$ coming into the final gate is $|I| + 1$, which is a majority.
On the other hand, if $i \notin I$, then each gate pair $G_i$ for all $i \in n$
will have only one member gate outputting $1$. Hence, the sum of the inputs
coming into the final gate is $|I|$, which is not a majority.

This concludes the construction.

\end{document}
