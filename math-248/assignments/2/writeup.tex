\documentclass[letterpaper,11pt]{article}

\author{Jacob Thomas Errington (260636023)}
\title{Assignment \#2\\Honours advanced calculus -- MATH 248}
\date{20 October 2016}

\usepackage[margin=2.0cm]{geometry}
\usepackage{amsmath,amsthm,amssymb}

\newtheorem{prop}{Proposition}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\d}[1]{\frac{\mathrm{d}}{\mathrm{d}#1}}

\begin{document}

\maketitle

\begin{enumerate}
    \item
        Let $f : \R^2 \to \R$ be defined by
        \begin{equation*}
            f(x, y) = \begin{cases}
                xy &\quad\text{if } -|x| < y < |x| \\
                0  &\quad\text{else}
            \end{cases}
        \end{equation*}

        \begin{enumerate}
            \item
                First, we will compute the first partial derivatives.
                \begin{align*}
                    \partial_x f &= \begin{cases}
                        y &\quad\text{if }
                            -|x| < y < |x| \lor x = 0\\
                        0 &\quad\text{if }
                            y > |x| \lor y < -|x|
                    \end{cases} \\
                    \partial_y f &= \begin{cases}
                        x &\quad\text{if }
                            -|x| < y < |x| \\
                        0 &\quad\text{if }
                            y > |x| \lor y < -|x| \lor x = 0
                    \end{cases}
                \end{align*}

                Notice that these partial derivatives are undefined along the
                boundary between the two functions that make up $f$, except at
                the origin. $f$ is discontinuous along the boundary, due to the
                jump from $xy \neq 0$ to $0$, but at the origin, there is no
                such jump. This is due to the fact that one of the factors in
                $xy$ will be zero as we approach the origin along the standard
                basis vectors. Consequently, the limit that defines the partial
                derivative will converge. However, the question of \emph{what}
                it converges to depends on how exactly the origin is
                approached.

                In $\partial_x f$, the origin is approached along the basis
                vector $(1, 0)$, so the limit that defines the partial
                derivative
                \begin{equation*}
                    \lim_{t \to 0} \frac{(x + t)y - xy}{t}
                    = \lim_{t \to 0} \frac{ty}{t}
                    = y
                \end{equation*}
                converges to $y$.

                In $\partial_y f$ however, the origin is approached along the
                basis vector $(0, 1)$, so the limit that defines the partial
                derivative converges to $0$.

                That is why the $x = 0$ case gives $0$ in $\partial_y$, but
                gives $y$ in the $\partial_x$ case. Of course, \emph{at} the
                origin, the function values of both partial derivatives agree.

                Now let us look at the second partial derivatives.
                \begin{align*}
                    \partial_y \partial_x f &= \begin{cases}
                        1 &\quad\text{if }
                            -|x| < y < |x| \lor x = 0\\
                        0 &\quad\text{if }
                            y > |x| \lor y < -|x|
                    \end{cases} \\
                    \partial_x \partial_y f &= \begin{cases}
                        1 &\quad\text{if }
                            -|x| < y < |x| \\
                        0 &\quad\text{if }
                            y > |x| \lor y < -|x| \lor x = 0
                    \end{cases}
                \end{align*}

                Although these two functions appear suspiciously similar, they
                are different in one major way,
                \begin{equation*}
                    1 = \partial_y \partial_x f(0, 0)
                    \neq
                    \partial_x \partial_y f(0, 0) = 0
                \end{equation*}
                due to the change in placement of the $x = 0$ case in the
                definition.

            \item
                Although this difference appears to contradict the theorem on
                the symmetricity of the Hessian, it does not; the theorem
                requires that the second partial derivatives exist and be
                continuous in some box domain (more generally in some
                neighbourhood). It is not hard to see that there is no
                neighbourhood containing the origin in which these second
                partial derivatives would be continuous.
        \end{enumerate}

    \item
        Consider $f : \R \to \R$
        \begin{equation*}
            f(x) = \begin{cases}
                \frac{x}{2} + x^2 \sin{\frac{1}{x}}
                    &\quad\text{if } x \neq 0 \\
                0   &\quad\text{if } x = 0
            \end{cases}
        \end{equation*}

        First let's ensure that $f$ is continuous everywhere. The only place
        where we might have problems is at $x = 0$.
        \begin{equation*}
            \lim_{x \to 0}\left(\frac{x}{2} + x^2 \sin{\frac{1}{x}}\right)
            = 0 + \lim_{x \to 0} {x^2 \sin{\frac{1}{x}}}
            = 0
        \end{equation*}
        which follows from a straightforward application of the Squeeze Theorem
        since $-1 \leq \sin{x} \leq 1 \forall x$.

        \begin{enumerate}
            \item
                The value $f^\prime(0)$ can be computed using the definition of
                the derivative.
                \begin{equation*}
                    \lim_{x \to y} {
                        \frac{x^2 \sin{\frac{1}{x}} - y^2 \sin{\frac{1}{y}}}{
                            x - y
                        }
                    }
                    = \lim_{x \to 0} {
                        \frac{x^2 \sin{\frac{1}{x}} - 0}{x}
                    }
                    = \lim_{x \to 0} {x \sin{\frac{1}{x}}}
                    = 0
                \end{equation*}

                But this isn't quite the derivative we're looking for. It's a
                part of it though. It suffices to add
                \begin{equation*}
                    \d{x} \frac{x}{2} = \frac{1}{2}
                \end{equation*}

                Consequently, the sum of these derivatives is
                $\frac{1}{2} \neq 0$, as required.

            \item
                Using the regular methods of differentiation, we can find an
                expression for the derivative of $f$ when $x \neq 0$.
                \begin{equation*}
                    f^\prime(x)
                        = \frac{1}{2} + 2x\sin{\frac{1}{x}} - \cos{\frac{1}{x}}
                \end{equation*}

                \begin{prop}
                    The function $f$ is not invertible on any interval
                    $(-r, r)$ for $r > 0$.
                \end{prop}

                \begin{proof}
                    Since $0 < \frac{1}{x} < \infty$ and continuous on
                    the interval $(0, 1)$, we can select $x$ so that
                    $\frac{1}{x} = k\pi$ for $k \in \Z$. What's more is that
                    this $k$ can be made arbitrarily large, by taking values of
                    $x$ arbitrarily small. These $x$ values can become so small
                    that the term $2x\sin{\frac{1}{x}}$ vanishes, but the term
                    $\cos{\frac{1}{x}} = -1$. Consequently, $f^\prime(x) < 0$
                    for these very small values of $x$. However, we can use
                    precisely the same argument instead selecting $x$ very
                    small but so that $\frac{1}{x} = 2k\pi$. Thus, the $\sin{}$
                    term vanishes, but the $\cos{}$ term is equal to $1$, and
                    the derivative is positive.

                    No matter how much the interval $(-r, r)$ is contracted, we
                    can still take an even smaller $x$ that will make the
                    $\cos{}$ term have whatever value we like within the
                    interval $[-1,1]$.  Intuitively, what this means is that
                    the derivative oscillates infinitely frequently near the
                    origin.

                    Because the derivative changes sign, this indicates a local
                    minimum or maximum in the function $f$. In a neighbourhood
                    of these extrema, the Intermediate Value Theorem guarantees
                    that we will have the same $y$ value to the left and to the
                    right of the extremum, because $f$ is continuous.
                    Therefore, $f$ is not invertible on any interval containing
                    the origin.
                \end{proof}

            \item
                This does not contradict the Inverse Function Theorem. The
                theorem requires that the function be continuously
                differentiable on the domain in which to apply the theorem.
                However, $f^\prime(0) = \frac{1}{2}$ but
                $\lim_{x \to 0} {f^\prime(x)}$ does not exist (because the
                $\cos{}$ term does not approach a finite value as $x \to 0$).
                Hence, the Inverse Function Theorem does not apply.
        \end{enumerate}
\end{enumerate}

\end{document}
