\documentclass[letterpaper,11pt]{article}

\usepackage[margin=2.0cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}

\newtheorem{prop}{Proposition}

\author{Jacob Thomas Errington (260636023)}
\title{Assignment 1\\Honours Advanced Calculus -- MATH 248}
\date{27 September 2016}

\newcommand{\diff}[1]{\,\text{differentiable at}\,#1}
\newcommand{\R}{\mathbb{R}}
\newcommand{\as}[1]{\quad\text{as}\;#1}
\newcommand{\inv}[1]{\frac{1}{#1}}

\begin{document}

\maketitle

\begin{enumerate}
    \item
        \begin{enumerate}
            \item
                The function $f(x) = \sin{(\frac{1}{x})}$ is not continuous at
                $x = 0$, since $\frac{1}{x} \to \infty$ as $x \to 0$, and
                $\lim_{x\to\infty} {sin(x)}$ does not exist.

            \item
                The function $f(x) = x \sin{(\frac{1}{x})}$ is continuous at
                $x = 0$. Suppose there is a sequence $\{x_i\} \to 0$.
                $f(\{x_i\}) \to 0$ as well, since the factor
                $\sin{(\frac{1}{x_i})}$ remains bounded within $[-1, 1]$ for
                any $i$, but the factor $x_i \to 0$. Consequently, the overall
                limit will tend to $0$.

                Furthermore, the function $f$ is not differentiable at $x=0$.
                Consider the limit that defines the notion of
                differentiability, specialized for $f$.
                \begin{equation*}
                    \lim_{x\to y}{
                        \frac
                        {x \sin{(\frac{1}{x})} - y \sin{(\frac{1}{y})}}
                        {x - y}
                    }
                \end{equation*}

                Since $y=0$, the limit becomes $\sin{(\frac{1}{x})}$ as
                $x \to 0$, which as discussed earlier, does not exist. Hence
                the function is not differentiable, despite being continuous.

            \item
                The function $f(x) = x^2 \sin{(\frac{1}{x})}$ is continuous by
                the same argument as given immediately above. However, it
                differs from the function above as $f$ here is differentiable.
                Consider the limit again: this time there will remain an
                additional factor $x$ in the numerator. Consequently, by the
                same argument that showed that $f$ is continuous in the
                previous problem, we find that the limit for differentiability
                of $f$ in this problem exists
        \end{enumerate}

    \item
        We wish to show that differentiability is a local property.

        \begin{prop}
            Differentiability is a local property, i.e. $f : K \to \R$ is
            differentiable at $y$ if and only if
            $g = f|_{(y-\epsilon, y+\epsilon)}$ is differentiable at $y$.
        \end{prop}

        \begin{proof}
            First we show the forwards direction. Using the sequential
            criterion, this is extremely straightforward. Since $f$ is
            differentiable at $y$, we know that for all sequences
            $\{x_i\} \subseteq K$ converging to values in $\R$, the limit
            \begin{equation*}
                \lim_{n\to\infty}{
                    \frac{f(x_i) - f(y)}{x - y} = f^\prime (x)
                }
            \end{equation*}
            exists and defines the derivative of $f$ at $y$. If we retrict our
            attention to those sequences that are bounded within the domain of
            $g$, then of course $f$ is differentiable there as well and the
            associated limits of those sequences converge. Since the values in
            those limits are bounded, and $f$ operating on that restricted
            domain is simply $g$, we can conclude that $g$ is differentiable
            there as well, by the sequential criterion.

            For the reverse direction, we will employ a similar argument using
            the sequential criterion of differentiability. Assume that $g$ is
            differentiable at $y$. Consider an arbitrary sequence
            $\{x_i\} \subseteq K$ converging to $y$. Now consider
            $\lim_{n\to\infty}{\frac{f(x_i) - g(y)}{x - y}}$. We want to show
            that this limit exists. By definition of a convergent sequence,
            for any $\epsilon > 0$, there exists some index $k$ such that
            $|x_k - y| < \epsilon$. Consequently, we instantiate this universal
            statement taking its $\epsilon$ to be the $\epsilon$ of the
            restricted domain of $g$ and consider a new sequence
            $\{x_i^\prime\}$ constructed from $\{x_i\}$ by dropping all items
            before $k$. This restricted sequence can be used applied to
            instantiate the sequential criterion for $g$
            \begin{equation*}
                \lim_{n\to\infty}{
                    \frac{g(x_i^\prime) - g(y)}{x - y}
                }
            \end{equation*}
            However, we know that $g$ is defined by $f$ in the domain
            $(y-\epsilon, y+\epsilon)$ within which lie each $x_i^\prime$, so
            we may replace $g$ with $f$ to obtain the convergent limit
            \begin{equation*}
                \lim_{n\to\infty}{
                    \frac{f(x_i^\prime) - f(y)}{x - y}
                }
            \end{equation*}
            This is almost the sequential criterion for $f$. All that remains
            is to return to the sequence $x_i$ instead of $x_i^\prime$: we know
            that for any convergent sequent $x_i$, prepending finitely many
            elements to it does not affect its convergence. The relationship
            between our initial sequence $x_i$ and our constructed sequence
            $x_i^\prime$ is precisely that there is some finite number ($k$ to
            be exact) of initial elements in $x_i$ before it is the same as
            $x_i^\prime$. Thus, we can replace $x_i^\prime$ with $x_i$ and
            obtain the convergent limit
            \begin{equation*}
                \lim_{n\to\infty}{
                    \frac{f(x_i) - f(y)}{x - y}
                }
            \end{equation*}
            which is precisely the sequential criterion for $f$, showing that
            $f$ is differentiable at $y$.
        \end{proof}

    \item Uniqueness of the derivative.

        \begin{prop}
            Let $f : (a, b) \to \R^n$ and $y \in (a, b)$. If
            $\lambda_1, \lambda_2, \mu_1, \mu_2 \in \R^n$ satisfy
            \begin{align}
                f(x) &= \lambda_1 + \mu_1(x-y) + o(x-y) \as{x \to y}
                \label{eq:uniqderiv1} \\
                f(x) &= \lambda_2 + \mu_2(x-y) + o(x-y) \as{x \to y}
                \label{eq:uniqderiv2}
            \end{align}
            then $\lambda_1 = \lambda_2 = f(y)$ and
            $\mu_1 = \mu_2 = f^\prime(y)$.
        \end{prop}

        \begin{proof}
            First, we will show that $\lambda_1 = \lambda_2 = f(y)$. To do so,
            is simple. Simply remark that as $x \to y$, $f(x) \to f(y)$ and the
            non-$\lambda$ terms in each equation vanish, so
            $\lambda_1 = \lambda_2 = f(y)$. Thus we have
            \begin{align*}
                f(x) &= f(y) - \mu_1(x-y) + o(x-y) \as{x \to y} \\
                f(x) &= f(y) - \mu_2(x-y) + o(x-y) \as{x \to y}
            \end{align*}
            Then, due to the lemma seen in class on equivalent definitions of
            differentiability of vector-valued functions, we can immediately
            conclude that $\mu_1 = \mu_2$ is the derivative of $f$ at $y$.
        \end{proof}

    \item Consider the $p$-quasinorm of $x \in \R^n$:
        \begin{equation*}
            |x|_p = \left( \sum_{k=1}^n |x_k|^p\right)^\frac{1}{p}
        \end{equation*}

        \begin{prop}
            $|x|_p = 0 \implies x = 0$
        \end{prop}

        \begin{proof}
            First, let us show the case where $p$ is a finite positive real.

            We will show the contrapositive. We wish to show that
            $|x|_p \neq 0$. Suppose $x \neq 0$. Then $\exists k \in [n]$ such
            that $x_k \neq 0$. Hence, $\sum_{k=1}^n {|x_k|^p}$ is positive,
            since the summand is non-negative for all $k$ and, at least for
            one $k$, the summand is positive. Taking the $p$-th root of the sum
            does not make it zero, so $|x|_p$ is positive and thus nonzero, as
            required.

            Next, let us consider the case $|x|_\infty$.

            We will again show the contrapositive. Again, there is at least one
            $k$ such that $x_k \neq 0$. Consequently, $\max_{k=1}^n |x_k|$ is
            positive, so $|x|_\infty \neq 0$ as required.
        \end{proof}

        \begin{prop}
            $|\lambda x|_p = |\lambda| \cdot |x|_p$ for $\lambda \in \R$.
        \end{prop}

        \begin{proof}
            First, let us show the case where $p$ is finite.

            By the definition of the $p$-quasinorm, we have
            \begin{equation*}
                \left( \sum_{k=1}^n |\lambda x_k|^p \right) ^ \frac{1}{p}
            \end{equation*}

            We can rewrite the summand to $|\lambda|^p |x_k|^p$ and factor out
            the $\lambda$ factor from the sum. The $p$-th root distributes over
            the the product of the $\lambda$ factor and the sum, and will
            cancel the $p$-th power on the $\lambda$ factor, leaving us with
            \begin{equation*}
                |\lambda| \left ( \sum_{k=1}^n |x_k|^p \right) ^ \frac{1}{p}
            \end{equation*}
            which is $|\lambda| \cdot |x|_p$, as required.

            Next, let us consider the case $|\lambda x|_\infty$.

            We have $\max_{k=1}^n |\lambda x_k|$, which can be rewritten
            $\max_{k=1}^n (|\lambda| \cdot |x_k|)$. If
            \begin{equation*}
                |\lambda| \cdot |x_k| \geq |\lambda| \cdot |x_i|\,
                \forall i \in [n]
            \end{equation*}
            then we can simply divide through by $|\lambda|$, allowing us to
            rewrite the expression we're working on to
            \begin{equation*}
                |\lambda| \max_{k=1}^n |x_k| = |\lambda| \cdot |x|_\infty
            \end{equation*}
            as required.
        \end{proof}

        \begin{prop}
            Let $x, y \in \R^n$, and $\alpha = \min\{1, p\}$. Then,
            \begin{equation*}
                |x + y|_p^\alpha \leq |x|_p^\alpha + |y|_p^\alpha
            \end{equation*}
        \end{prop}

        \begin{proof}
            We must consider three cases.

            First, suppose $0 < p \leq 1$. The $p$-th root and the $p$-th power
            will cancel.
            \begin{equation*}
                |x + y|_p^p
                = \left(
                \left( \sum_{k=1}^n |x_k + y_k|^p \right) ^ \frac{1}{p}
                \right) ^ p
                = \sum_{k=1}^n |x_k + y_k|^p
            \end{equation*}
            Then, we apply the triangle inequality in each term of the sum, and
            use the fact that $\forall a,b > 0$ and $0 < p \leq 1$,
            $(a + b)^p \leq a^p + b^p$ to obtain the following.
            \begin{align*}
                \sum_{k=1}^n |x_k + y_k|^p
                &\leq \sum_{k=1}^n (|x_k| + |y_k|)^p \\
                &\leq \sum_{k=1}^n (|x_k|^p + |y_k|^p) \\
                &= \sum_{k=1}^n |x_k|^p + \sum_{k=1}^n |y_k|^p \\
                &= |x|_p^p + |y|_p^p
            \end{align*}
            as required.

            Next, suppose $1 < p < \infty$. We want to show
            \begin{equation*}
                \left(\sum_{k=1}^n |x_k + y_k|^p \right)^\frac{1}{p}
                \leq
                \left(\sum_{k=1}^n |x_k|^p \right)^\frac{1}{p}
                +
                \left(\sum_{k=1}^n |y_k|^p \right)^\frac{1}{p}
            \end{equation*}

            Here is our deduction.
            \begin{align*}
                \left(\sum_{k=1}^n |x_k + y_k|^p\right)^\inv{p}
                &\leq \left(\sum_{k=1}^n (|x_k| + |y_k|)^p\right)^\inv{p} \\
                &\leq \left(\sum_{k=1}^n |x_k|^p\right)^\inv{p}
                + \left(\sum_{k=1}^n |y_k|^p\right)^\inv{p}
            \end{align*}
            since $\sum (|x_k| + |y_k|)^p \geq \sum |x_k|^p + \sum |y_k|^p$.

            Finally, suppose $p = \infty$. We want to show
            \begin{equation*}
                \max_{k=1}^n |x_k + y_k|
                \leq \max_{k=1}^n |x_k| + \max_{k=1}^n |y_k|
            \end{equation*}

            Using the triangle inequality, we get
            \begin{equation*}
                \max_{k=1}^n |x_k + y_k| \leq \max_{k=1}^n (|x_k| + |y_k|)
            \end{equation*}

            Suppose that it is not the case that
            $\max_{k=1}^n (|x_k| + |y_k|)
            \leq \max_{k=1}^n |x_k| + \max_{k=1}^n |y_k|$. Then, for \emph{any}
            choice of $k_1$ and $k_2$,
            $|x_{k_1}| + |y_{k_2}| > \max_{k=1}^n (|x_k| + |y_k|)$. Of course,
            this is impossible because we could choose $k_1 = k_2 = k^\prime$
            where $k^\prime$ is such that it is the arg max of the RHS of the
            inequality.
        \end{proof}

        \begin{prop}
            $$|x|_\infty \leq |x|_p \leq n^\inv{p} |x|_\infty$$
        \end{prop}

        \begin{proof}
            First, we will show $|x|_\infty \leq |x|_p$ by induction on the
            number of dimensions $n$.

            Base case: $n = 1$. Clearly,
            \begin{equation*}
                \max \{ |x_1| \} = |x_1|
                \leq
                \left(|x_1|^p\right)^\inv{p} = |x_1|
            \end{equation*}

            Step case: assume
            $\max_{k=1}^n |x_k|
            \leq \left(\sum_{k=1}^n |x_k|^p \right)^\inv{p}$. We wish to add
            another component to the vector $x$, so that it has $n+1$
            components. Notice
            \begin{equation*}
                \max_{k=1}^{n+1} |x_k|
                = \max\{\max_{k=1}^n |x_k|,\, |x_{n+1}|\}
            \end{equation*}
            so there are two cases to consider.

            \begin{enumerate}
                \item The maximum component does not change. Then
                    \begin{equation*}
                        \max_{k=1}^{n+1} |x_k| = \max_{k=1}^n |x_k|
                        \leq \left(\sum_{k=1}^n |x_k|^p\right)^\inv{p}
                        \leq \left(\sum_{k=1}^{n+1} |x_k|^p\right)^\inv{p}
                    \end{equation*}
                    as required.

                \item The maximum component becomes $x_{n+1}$. Clearly,
                    \begin{equation*}
                        x_{n+1} = \left(|x_{n+1}|^p\right)^\inv{p}
                        \leq \left(\sum_{k=1}^{n+1} |x_k|^p\right)^\inv{p}
                    \end{equation*}
                    as required.
            \end{enumerate}

            Next, we will show the second inequality. This involves nothing
            more than a simple algebraic manipulation.
            \begin{equation*}
                \left(\sum_{k=1}^n |x_k|^p\right)^\inv{p}
                \leq
                \left(
                    \sum_{k=1}^n \left(\max_{i=1}^n |x_i|\right)^p
                \right)^\inv{p}
                = \left(n \left(\max_{i=1}^n |x_i|\right)^p\right)^\inv{p}
                = n^\inv{p} \max_{k=1}^n |x_k|
            \end{equation*}
        \end{proof}
\end{enumerate}

\end{document}
