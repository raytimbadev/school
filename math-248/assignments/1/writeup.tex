\documentclass[letterpaper,11pt]{article}

\usepackage[margin=2.0cm]{geometry}
\usepackage{amsmath,amssymb,amsthm}

\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}

\author{Jacob Thomas Errington (260636023)}
\title{Assignment 1\\Honours Advanced Calculus -- MATH 248}
\date{27 September 2016}

\newcommand{\diff}[1]{\,\text{differentiable at}\,#1}
\newcommand{\R}{\mathbb{R}}
\newcommand{\as}[1]{\quad\text{as}\;#1}
\newcommand{\inv}[1]{\frac{1}{#1}}
\renewcommand{\d}[1]{\frac{\text{d}}{\text{d}#1}}
\newcommand{\del}[1]{\frac{\partial}{\partial#1}}

\begin{document}

\maketitle

\begin{enumerate}
    \item
        \begin{enumerate}
            \item
                The function $f(x) = \sin{\left(\frac{1}{x}\right)}$ is not
                continuous at $x = 0$, since $\frac{1}{x} \to \infty$ as
                $x \to 0$, and $\lim_{x\to\infty} {\sin(x)}$ does not exist.

            \item
                The function $f(x) = x \sin{(\frac{1}{x})}$ is continuous at
                $x = 0$. Suppose there is a sequence $\{x_i\} \to 0$.
                $f(\{x_i\}) \to 0$ as well, since the factor
                $\sin{(\frac{1}{x_i})}$ remains bounded within $[-1, 1]$ for
                any $i$, but the factor $x_i \to 0$. Consequently, the overall
                limit will tend to $0$.

                Furthermore, the function $f$ is not differentiable at $x=0$.
                Consider the limit that defines the notion of
                differentiability, specialized for $f$.
                \begin{equation*}
                    \lim_{x\to y}{
                        \frac
                        {x \sin{(\frac{1}{x})} - y \sin{(\frac{1}{y})}}
                        {x - y}
                    }
                \end{equation*}

                Since $y=0$, the limit becomes $\sin{(\frac{1}{x})}$ as
                $x \to 0$, which as discussed earlier, does not exist. Hence
                the function is not differentiable, despite being continuous.

            \item
                The function $f(x) = x^2 \sin{(\frac{1}{x})}$ is continuous by
                the same argument as given immediately above. However, it
                differs from the function above as $f$ here is differentiable.
                Consider the limit again: this time there will remain an
                additional factor $x$ in the numerator. Consequently, by the
                same argument that showed that $f$ is continuous in the
                previous problem, we find that the limit for differentiability
                of $f$ in this problem exists
        \end{enumerate}

    \item
        We wish to show that differentiability is a local property.

        \begin{prop}
            Differentiability is a local property, i.e. $f : K \to \R$ is
            differentiable at $y$ if and only if
            $g = f|_{(y-\epsilon, y+\epsilon)}$ is differentiable at $y$.
        \end{prop}

        \begin{proof}
            First we show the forwards direction. Using the sequential
            criterion, this is extremely straightforward. Since $f$ is
            differentiable at $y$, we know that for all sequences
            $\{x_i\} \subseteq K$ converging to values in $\R$, the limit
            \begin{equation*}
                \lim_{n\to\infty}{
                    \frac{f(x_i) - f(y)}{x - y} = f^\prime (x)
                }
            \end{equation*}
            exists and defines the derivative of $f$ at $y$. If we retrict our
            attention to those sequences that are bounded within the domain of
            $g$, then of course $f$ is differentiable there as well and the
            associated limits of those sequences converge. Since the values in
            those limits are bounded, and $f$ operating on that restricted
            domain is simply $g$, we can conclude that $g$ is differentiable
            there as well, by the sequential criterion.

            For the reverse direction, we will employ a similar argument using
            the sequential criterion of differentiability. Assume that $g$ is
            differentiable at $y$. Consider an arbitrary sequence
            $\{x_i\} \subseteq K$ converging to $y$. Now consider
            $\lim_{n\to\infty}{\frac{f(x_i) - g(y)}{x - y}}$. We want to show
            that this limit exists. By definition of a convergent sequence,
            for any $\epsilon > 0$, there exists some index $k$ such that
            $|x_k - y| < \epsilon$. Consequently, we instantiate this universal
            statement taking its $\epsilon$ to be the $\epsilon$ of the
            restricted domain of $g$ and consider a new sequence
            $\{x_i^\prime\}$ constructed from $\{x_i\}$ by dropping all items
            before $k$. This restricted sequence can be used applied to
            instantiate the sequential criterion for $g$
            \begin{equation*}
                \lim_{n\to\infty}{
                    \frac{g(x_i^\prime) - g(y)}{x - y}
                }
            \end{equation*}
            However, we know that $g$ is defined by $f$ in the domain
            $(y-\epsilon, y+\epsilon)$ within which lie each $x_i^\prime$, so
            we may replace $g$ with $f$ to obtain the convergent limit
            \begin{equation*}
                \lim_{n\to\infty}{
                    \frac{f(x_i^\prime) - f(y)}{x - y}
                }
            \end{equation*}
            This is almost the sequential criterion for $f$. All that remains
            is to return to the sequence $x_i$ instead of $x_i^\prime$: we know
            that for any convergent sequent $x_i$, prepending finitely many
            elements to it does not affect its convergence. The relationship
            between our initial sequence $x_i$ and our constructed sequence
            $x_i^\prime$ is precisely that there is some finite number ($k$ to
            be exact) of initial elements in $x_i$ before it is the same as
            $x_i^\prime$. Thus, we can replace $x_i^\prime$ with $x_i$ and
            obtain the convergent limit
            \begin{equation*}
                \lim_{n\to\infty}{
                    \frac{f(x_i) - f(y)}{x - y}
                }
            \end{equation*}
            which is precisely the sequential criterion for $f$, showing that
            $f$ is differentiable at $y$.
        \end{proof}

    \item Uniqueness of the derivative.

        \begin{prop}
            Let $f : (a, b) \to \R^n$ and $y \in (a, b)$. If
            $\lambda_1, \lambda_2, \mu_1, \mu_2 \in \R^n$ satisfy
            \begin{align}
                f(x) &= \lambda_1 + \mu_1(x-y) + o(x-y) \as{x \to y}
                \label{eq:uniqderiv1} \\
                f(x) &= \lambda_2 + \mu_2(x-y) + o(x-y) \as{x \to y}
                \label{eq:uniqderiv2}
            \end{align}
            then $\lambda_1 = \lambda_2 = f(y)$ and
            $\mu_1 = \mu_2 = f^\prime(y)$.
        \end{prop}

        \begin{proof}
            First, we will show that $\lambda_1 = \lambda_2 = f(y)$. To do so,
            is simple. Simply remark that as $x \to y$, $f(x) \to f(y)$ and the
            non-$\lambda$ terms in each equation vanish, so
            $\lambda_1 = \lambda_2 = f(y)$. Thus we have
            \begin{align*}
                f(x) &= f(y) - \mu_1(x-y) + o(x-y) \as{x \to y} \\
                f(x) &= f(y) - \mu_2(x-y) + o(x-y) \as{x \to y}
            \end{align*}
            Then, due to the lemma seen in class on equivalent definitions of
            differentiability of vector-valued functions, we can immediately
            conclude that $\mu_1 = \mu_2$ is the derivative of $f$ at $y$.
        \end{proof}

    \item Consider the $p$-quasinorm of $x \in \R^n$:
        \begin{equation*}
            |x|_p = \left( \sum_{k=1}^n |x_k|^p\right)^\frac{1}{p}
        \end{equation*}

        \begin{prop}
            $|x|_p = 0 \implies x = 0$
        \end{prop}

        \begin{proof}
            First, let us show the case where $p$ is a finite positive real.

            We will show the contrapositive. We wish to show that
            $|x|_p \neq 0$. Suppose $x \neq 0$. Then $\exists k \in [n]$ such
            that $x_k \neq 0$. Hence, $\sum_{k=1}^n {|x_k|^p}$ is positive,
            since the summand is non-negative for all $k$ and, at least for
            one $k$, the summand is positive. Taking the $p$-th root of the sum
            does not make it zero, so $|x|_p$ is positive and thus nonzero, as
            required.

            Next, let us consider the case $|x|_\infty$.

            We will again show the contrapositive. Again, there is at least one
            $k$ such that $x_k \neq 0$. Consequently, $\max_{k=1}^n |x_k|$ is
            positive, so $|x|_\infty \neq 0$ as required.
        \end{proof}

        \begin{prop}
            $|\lambda x|_p = |\lambda| \cdot |x|_p$ for $\lambda \in \R$.
        \end{prop}

        \begin{proof}
            First, let us show the case where $p$ is finite.

            By the definition of the $p$-quasinorm, we have
            \begin{equation*}
                \left( \sum_{k=1}^n |\lambda x_k|^p \right) ^ \frac{1}{p}
            \end{equation*}

            We can rewrite the summand to $|\lambda|^p |x_k|^p$ and factor out
            the $\lambda$ factor from the sum. The $p$-th root distributes over
            the the product of the $\lambda$ factor and the sum, and will
            cancel the $p$-th power on the $\lambda$ factor, leaving us with
            \begin{equation*}
                |\lambda| \left ( \sum_{k=1}^n |x_k|^p \right) ^ \frac{1}{p}
            \end{equation*}
            which is $|\lambda| \cdot |x|_p$, as required.

            Next, let us consider the case $|\lambda x|_\infty$.

            We have $\max_{k=1}^n |\lambda x_k|$, which can be rewritten
            $\max_{k=1}^n (|\lambda| \cdot |x_k|)$. If
            \begin{equation*}
                |\lambda| \cdot |x_k| \geq |\lambda| \cdot |x_i|\,
                \forall i \in [n]
            \end{equation*}
            then we can simply divide through by $|\lambda|$, allowing us to
            rewrite the expression we're working on to
            \begin{equation*}
                |\lambda| \max_{k=1}^n |x_k| = |\lambda| \cdot |x|_\infty
            \end{equation*}
            as required.
        \end{proof}

        \begin{prop}
            Let $x, y \in \R^n$, and $\alpha = \min\{1, p\}$. Then,
            \begin{equation*}
                |x + y|_p^\alpha \leq |x|_p^\alpha + |y|_p^\alpha
            \end{equation*}
        \end{prop}

        \begin{proof}
            We must consider three cases.

            First, suppose $0 < p \leq 1$. The $p$-th root and the $p$-th power
            will cancel.
            \begin{equation*}
                |x + y|_p^p
                = \left(
                \left( \sum_{k=1}^n |x_k + y_k|^p \right) ^ \frac{1}{p}
                \right) ^ p
                = \sum_{k=1}^n |x_k + y_k|^p
            \end{equation*}
            Then, we apply the triangle inequality in each term of the sum, and
            use the fact that $\forall a,b > 0$ and $0 < p \leq 1$,
            $(a + b)^p \leq a^p + b^p$ to obtain the following.
            \begin{align*}
                \sum_{k=1}^n |x_k + y_k|^p
                &\leq \sum_{k=1}^n (|x_k| + |y_k|)^p \\
                &\leq \sum_{k=1}^n (|x_k|^p + |y_k|^p) \\
                &= \sum_{k=1}^n |x_k|^p + \sum_{k=1}^n |y_k|^p \\
                &= |x|_p^p + |y|_p^p
            \end{align*}
            as required.

            Next, suppose $1 < p < \infty$. We want to show
            \begin{equation*}
                \left(\sum_{k=1}^n |x_k + y_k|^p \right)^\frac{1}{p}
                \leq
                \left(\sum_{k=1}^n |x_k|^p \right)^\frac{1}{p}
                +
                \left(\sum_{k=1}^n |y_k|^p \right)^\frac{1}{p}
            \end{equation*}

            Here is our deduction.
            \begin{align*}
                \left(\sum_{k=1}^n |x_k + y_k|^p\right)^\inv{p}
                &\leq \left(\sum_{k=1}^n (|x_k| + |y_k|)^p\right)^\inv{p} \\
                &\leq \left(\sum_{k=1}^n |x_k|^p\right)^\inv{p}
                + \left(\sum_{k=1}^n |y_k|^p\right)^\inv{p}
            \end{align*}
            since $\sum (|x_k| + |y_k|)^p \geq \sum |x_k|^p + \sum |y_k|^p$.

            Finally, suppose $p = \infty$. We want to show
            \begin{equation*}
                \max_{k=1}^n |x_k + y_k|
                \leq \max_{k=1}^n |x_k| + \max_{k=1}^n |y_k|
            \end{equation*}

            Using the triangle inequality, we get
            \begin{equation*}
                \max_{k=1}^n |x_k + y_k| \leq \max_{k=1}^n (|x_k| + |y_k|)
            \end{equation*}

            Suppose that it is not the case that
            $\max_{k=1}^n (|x_k| + |y_k|)
            \leq \max_{k=1}^n |x_k| + \max_{k=1}^n |y_k|$. Then, for \emph{any}
            choice of $k_1$ and $k_2$,
            $|x_{k_1}| + |y_{k_2}| > \max_{k=1}^n (|x_k| + |y_k|)$. Of course,
            this is impossible because we could choose $k_1 = k_2 = k^\prime$
            where $k^\prime$ is such that it is the arg max of the RHS of the
            inequality.
        \end{proof}

        \begin{prop}
            $$|x|_\infty \leq |x|_p \leq n^\inv{p} |x|_\infty$$
        \end{prop}

        \begin{proof}
            First, we will show $|x|_\infty \leq |x|_p$ by induction on the
            number of dimensions $n$.

            Base case: $n = 1$. Clearly,
            \begin{equation*}
                \max \{ |x_1| \} = |x_1|
                \leq
                \left(|x_1|^p\right)^\inv{p} = |x_1|
            \end{equation*}

            Step case: assume
            $\max_{k=1}^n |x_k|
            \leq \left(\sum_{k=1}^n |x_k|^p \right)^\inv{p}$. We wish to add
            another component to the vector $x$, so that it has $n+1$
            components. Notice
            \begin{equation*}
                \max_{k=1}^{n+1} |x_k|
                = \max\{\max_{k=1}^n |x_k|,\, |x_{n+1}|\}
            \end{equation*}
            so there are two cases to consider.

            \begin{enumerate}
                \item The maximum component does not change. Then
                    \begin{equation*}
                        \max_{k=1}^{n+1} |x_k| = \max_{k=1}^n |x_k|
                        \leq \left(\sum_{k=1}^n |x_k|^p\right)^\inv{p}
                        \leq \left(\sum_{k=1}^{n+1} |x_k|^p\right)^\inv{p}
                    \end{equation*}
                    as required.

                \item The maximum component becomes $x_{n+1}$. Clearly,
                    \begin{equation*}
                        x_{n+1} = \left(|x_{n+1}|^p\right)^\inv{p}
                        \leq \left(\sum_{k=1}^{n+1} |x_k|^p\right)^\inv{p}
                    \end{equation*}
                    as required.
            \end{enumerate}

            Next, we will show the second inequality. This involves nothing
            more than a simple algebraic manipulation.
            \begin{equation*}
                \left(\sum_{k=1}^n |x_k|^p\right)^\inv{p}
                \leq
                \left(
                    \sum_{k=1}^n \left(\max_{i=1}^n |x_i|\right)^p
                \right)^\inv{p}
                = \left(n \left(\max_{i=1}^n |x_i|\right)^p\right)^\inv{p}
                = n^\inv{p} \max_{k=1}^n |x_k|
            \end{equation*}
        \end{proof}

    \item Equivalent statements.

        \begin{prop}
            Let $f : F \to \R^n$, $h : K \to \R$, $0 < p < \infty$, and
            $y \in \R$. Then,
            \begin{equation*}
                \lim_{x \to y} \frac{|f(x)|_\infty}{|h(x)|} = 0
                \iff
                \lim_{x \to y} \frac{|f(x)|_p}{|h(x)|} = 0
            \end{equation*}
        \end{prop}

        \begin{proof}
            First, we will show the backwards direction: suppose
            $\lim_{x \to y} \frac{|f(x)|_p}{|h(x)|} = 0$.

            We have already shown that $|x|_\infty \leq |x|_p$, so
            $|f(x)|_\infty \leq |f(x)|_p$. Therefore, we have
            \begin{equation*}
                0
                \leq
                \lim_{x\to y} \frac{|f(x)|_\infty}{|h(x)|}
                \leq
                \lim_{x\to y} \frac{|f(x)|_p}{|h(x)|} = 0
            \end{equation*}

            By the Squeeze Theorem, we conclude that
            \begin{equation*}
                \lim_{x \to y} \frac{|f(x)|_\infty}{|h(x)|} = 0
            \end{equation*}

            Next, we will show the forwards direction: suppose
            $\lim_{x \to y} \frac{|f(x)|_\infty}{|h(x)|} = 0$.

            We have already shown that $|x|_p \leq n^\inv{p} |x|_\infty$, so
            $|f(x)|_p \leq n^\inv{p} |f(x)|_\infty$. Therefore, we have
            \begin{equation*}
                0
                \leq
                \lim_{x \to y} \frac{|f(x)|_p}{|h(x)|}
                \leq
                n^\inv{p} \lim_{x \to y} \frac{|f(x)|_\infty}{|h(x)|} = 0
            \end{equation*}

            Again by the Squeeze Theorem, we conclude that
            \begin{equation*}
                \lim_{x \to y} \frac{|f(x)|_p}{|h(x)|} = 0
            \end{equation*}
        \end{proof}

    \item Definitions for continuity and differentiability using the
        $p$-quasinorm.

        \begin{definition}
            A vector-valued function $f : K \to \R^n$, with $K \subseteq \R$,
            is \emph{continuous} at $y \in K$ if
            \begin{equation*}
                \lim_{x \to y} |f(x) - f(y)|_p = 0
            \end{equation*}
        \end{definition}

        \begin{definition}
            A vector-valued function $f : K \to \R^n$, with $K \subseteq \R$,
            is \emph{differentiable} at $y \in K$ if
            \begin{equation*}
                \lim_{x \to y} \frac{|f(x) - f(y)|_p}{|x - y|} = 0
            \end{equation*}
        \end{definition}

        These definitions are equivalent to the little-$o$ definitions of
        continuity and differentiability seen in class (using the infinity
        norm) due to the above proof showing equivalence of infinity norm
        little-$o$ and $p$-quasinorm little-$o$.

    \item Let $A : (a, b) \to \R^{m \times n}$ and $x : (a, b) \to \R^n$ be
        both differentiable at $t \in (a, b)$.

        \begin{prop}
            The product $Ax : (a, b) \to \R^m$, defined by $s \mapsto A(s)x(s)$
            is differentiable at $t$, with
            \begin{equation*}
                (Ax)^\prime (t) = A^\prime (t) x(t) + A(t) x^\prime (t)
            \end{equation*}
        \end{prop}

        \begin{proof}
            First, we will show that the product is differentiable.

            Let $f_{A(t)} : \R^n \to \R^m$ denote the function represented by
            the matrix $A(t)$. We argue that it is differentiable because each
            of its partial derivatives exist and are differentiable in $\R^n$,
            since each function to take the derivative of is simply a linear
            function, it is differentiable and its derivative is continuous.

            Now we will find the derivative. To do so, we will look at the
            components of $Ax$. The $k$th component of $Ax$ is given by the
            formula for multiplying a matrix by a vector.
            \begin{equation*}
                ((Ax)(s))_k = \sum_{i=1}^n (A(s))_{ki} x(s)_i
            \end{equation*}
            Notice that for a fixed $i$, $(A(s))_{ki}$ can be thought of as a
            function $\R \to \R$; the same applies to $x(s)_i$. Hence, we can
            use the single-variable product rule to find the derivative of the
            $k$th component.
            \begin{align*}
                \d{s} \sum_{i=1}^n (A(s))_{ki} x(s)_i
                &= \sum_{i=1}^n { \d{s} (A(s))_{ki} x(s)_i } \\
                &= \sum_{i=1}^n \left(
                    ((A(s))_{ki})^\prime x(s)_i + (A(s))_{ki} (x(s)_i)^\prime
                \right)\\
                &= \sum_{i=1}^n \left(
                    (A^\prime (s))_{ki} x(s)_i + (A(s))_{ki} (x^\prime (s))_i
                \right)\\
                &= \sum_{i=1}^n (A^\prime (s))_{ki} x(s)_i
                + \sum_{i=1}^n (A(s))_{ki} (x^\prime (s))_i \\
            \end{align*}
            But these last two terms are precisely the $k$th component of
            $A^\prime (s) x(s)$ and $A(s) x^\prime (s)$, which is what we were
            seeking to show.
        \end{proof}

    \item Differentiability along restrictions.

        \begin{prop}
            The function $u : \R^2 \to \R$ defined by
            \begin{equation*}
                u(x, y) = \begin{cases}
                    1 &\text{if } e^\frac{-2}{x} \leq y \leq e^\frac{-1}{x} \\
                    0 &\text{else}
                \end{cases}
            \end{equation*}
            is discontinuous at $0 \in \R^2$, but is continuous along every
            restriction to a polynomial curve.
        \end{prop}

        In particular, approaching along a curve bounded between the two
        exponential curves will produce a limit of $1$ at the origin, but
        approaching along any polynomial curve will produce a limit of $0$

        \begin{proof}
            First, the function $u$ is discontinuous since approaching the
            origin along the sequence
            $\{(\frac{1}{n}, e^\frac{-2}{\frac{1}{n}})\}$ as $n \to \infty$
            results in a limit of $1$, but approaching along the sequence
            $\{(\frac{1}{n}, 0)\}$ as $n \to \infty$ results in a limit of $0$.

            Next, take an arbitrary polynomial $p(t)$. Several cases arise.
            \begin{enumerate}
                \item If $p$ is the zero polynomial, then
                    $u(t, p(t)) = u(t, 0) \to 0$ as
                    $t \to 0$ and $u(0, 0) = 0$, so $u$ is continuous along
                    this curve.

                \item If $p(0) \neq 0$, then as $t \to 0$, $(t, p(t))$ does not
                    approach $(0, 0)$, so $u$ is continuous at $(0, 0)$ along
                    this curve as well.

                \item If $p(0) = 0$, then we can bound the polynomial in a
                    neighbourhood around the zero. Specifically, let $n$ be the
                    multiplicity of the zero $t = 0$. Then there exist $c > 0$
                    and $d > 0$ such that for $t \in [0, d]$ either
                    \begin{align*}
                        p(t) &\geq ct^n \quad \text{or}\\
                        p(t) &\leq -ct^n
                    \end{align*}

                    In the first case, the polynomial is above both exponential
                    curves as $t \to 0$, since the exponential curves approach
                    $0$ exponentially fast, must faster than $ct^n$. Hence the
                    limit along the polynomial curve is again $0$, which
                    corresponds with the function value at $(0, 0)$. Thus $u$
                    is continuous along this polynomial curve as well.

                    In the second case, there is nothing to show since the
                    polynomial is negative and does not interact whatsoever
                    with the exponential curves. Again the limit value will
                    correspond with the function value, so $u$ is continuous
                    along this curve.
            \end{enumerate}

            Thus, since the choice of $p(t)$ was arbitrary, we can generalize
            to all polynomial curves; the function $u$ we have constructed is
            discontinuous at $0 \in \R^2$, but is continuous along any
            polynomial curve through the origin.
        \end{proof}

    \item Continuity of multivariate functions.

        \begin{enumerate}
            \item The Weierstrass definition of continuity for multivariate
                functions.

                \begin{prop}
                    For all $h : [0, \infty) \to \R$ such that $h(t) = o(1)$ as
                    $t \to 0$, if $K \subseteq \R^n$ and $f : K \to \R$ such
                    that
                    \begin{equation*}
                        |f(x) - f(y)| \leq h(|x - y|_\infty)
                        \quad \forall x, y \in K
                    \end{equation*}
                    then $f$ is continuous on $K$.
                \end{prop}

                \begin{proof}
                    Take an arbitrary sequence $S = \{x^{(i)}\} \subseteq K$
                    converging to $y \in K$. We want to show that $f$ is
                    continuous, i.e.  $\{f(x^{(i)})\} \to f(y)$.

                    Since $|f(x) - f(y)| \leq h(|x - y|_\infty)$ for all
                    $x, y \in K$, we can replace $x$ here with any element of
                    $S$ to get
                    \begin{equation*}
                        0 \leq |f(x^{(i)}) - f(y)| < h(|x^{(i)} - y|_\infty)
                    \end{equation*}
                    for any $i$.

                    Notice that since $\{x^{(i)}\} \to y$,
                    $\{|x^{(i)} - y|_\infty\} \to 0$ as $i \to \infty$, so
                    $\{h(|x^{(i)} - y|_\infty\} \to 0$ as $i \to \infty$
                    because $h(t) = o(1)$.

                    By the Squeeze Theorem,
                    \begin{equation*}
                        \lim_{i\to\infty} {|f(x^{(i)} - f(y)|} = 0
                    \end{equation*}
                    so $\{f(x^{(i)})\} \to f(y)$, and $f(x) = f(y) + o(1)$.

                    This shows that $f$ is continuous.
                \end{proof}

            \item Continuity of the $p$-quasinorm.

                \begin{prop}
                    Consider the function $f : \R^n \to \R$ defined by the
                    $p$-quasinorm, $x \mapsto |x|_p$. $f$ is continuous, for
                    $0 < p \leq \infty$.
                \end{prop}

                \begin{proof}
                    First suppose $0 < p \leq 1$. Then,
                    \begin{equation*}
                        |x|_p = \sum_{k=1}^n |x_k|^p
                    \end{equation*}

                    Take an arbitrary convergent sequence $\{x^{(i)}\} \to y$
                    as $i \to \infty$.
                    To show continuity of $f$ at $y$, we must show that
                    $\{|x^{(i)}|_p\} \to |y|_p$ as $i \to \infty$. This
                    deduction follows mainly from the properties of sequences.
                    \begin{align*}
                        \lim_{i\to\infty} \{|x^{(i)}|_p\}
                        &= \lim_{i\to\infty} {
                            \left\{ \sum_{k=1}^n |x_k^{(i)}|^p \right\}
                        } \\
                        &= \sum_{k=1}^n {
                            \lim_{i\to\infty} {
                                \{|x_k^{(i)}|^p\}
                            }
                        } \\
                        &= \sum_{k=1}^n {
                            \lvert
                            \lim_{i\to\infty} \{ x_k^{(i)} \}
                            \rvert
                        }^p \\
                        &= \sum_{k=1}^n {|y_k|^p} \\
                        &= |y|_p
                    \end{align*}

                    The deduction is the same for $1 < p < \infty$, except that
                    we ``push'' the sequence inside the $p$-th root before
                    proceeding the same way.

                    The deduction is also the same for the maximum norm
                    $|\cdot|_\infty$, except that instead of ``pushing'' the
                    sequence inside the sum, we move into into the $\max{}$.
                \end{proof}
        \end{enumerate}

    \item
        \begin{enumerate}
            \item Power of an absolute value of a product.

                Consider the function $f : \R^2 \to \R$ defined by
                \begin{equation*}
                    f(x, y) = \begin{cases}
                        |xy|^\alpha &\quad\text{if } xy \neq 0 \\
                        0           &\quad\text{if } xy = 0
                    \end{cases}
                \end{equation*}

                We wish to find the values of the parameter $\alpha$ so that
                $f$ is continuous everywhere. Using the fact that the existence
                and continuity of the partial derivatives implies the
                differentiability of the function, it is easy to see that the
                only place there might be a problem is when $xy = 0$, since the
                absolute value function is not differentiable there.

                First, if $\alpha = 0$, then $f(x, y) = 1$ everywhere except
                when $xy = 0$, when $f(x, y) = 0$. Since $f$ is discontinuous
                in this case at $0$, it is not differentiable.

                Next, if $\alpha \neq 0$, we can find the partial derivatives
                of $f$ for $xy \neq 0$ using the chain rule and the power rule,
                considering $\d{x} {|x|} = \frac{|x|}{x}$, illustrating that
                the derivative is undefined at $x = 0$.
                \begin{align*}
                    \del{x} {|xy|^\alpha}
                    &= |y|^\alpha \d{x} |x|^\alpha \\
                    &= |y|^\alpha \alpha |x|^{\alpha - 1} \frac{|x|}{x} \\
                    &= \alpha \frac{|xy|^\alpha}{x}
                \end{align*}

                The limit of the partial derivative as $(x, y) \to (0, 0)$
                diverges at $x = 0$ if and only if the denominator approaches
                $0$ faster than the numerator does. This happens precisely when
                its degree exceeds the degree of the numerator, which happens
                for $0 < \alpha < \frac{1}{2}$. If
                $\frac{1}{2} < \alpha < \infty$, the numerator will approach
                zero faster than the denominator, and the value of the
                partial derivative will approach $0$. If $\alpha < 0$, then the
                numerator is just the constant $\alpha$, so the partial
                derivative will diverge.

            \item Differentiability of vector operations.

                Consider the function $f : \R^n \to \R$ defined by
                \begin{equation*}
                    x \mapsto x^T Ax + b^T x
                    = \sum_{i,k=1}^n A_{ik}x_ix_k + \sum_{i=1}^n b_i x_i
                \end{equation*}
                for some fixed $A \in \R^{n \times n}$ and $b \in \R^n$.

                It is obvious that $f$ is differentiable, because it produces a
                polynomial, and all polynomials are differentiable.

                To compute the derivative of $f$, we must find each of its
                partial derivatives. Suppose we wish to find the $p$th partial
                derivative of $f$. Any term not involving $x_p$ will vanish, so
                $\del{x_p} {\sum_{i=1}^n {b_i x_i}} = b_p$. As for the other
                sum, we have to watch out for $i = p$, and $j = p$. When
                $i = j = p$, then we have a quadratic term:
                \begin{equation*}
                    \del{x_p} {x_p A_{pp} x_p} = 2 A_{pp} x_p
                \end{equation*}
                This is a term in our $p$th partial derivative.

                Then, as $i$ runs from $1$ to $n$, skipping $p$, we have these
                terms if $j = p$:
                \begin{equation*}
                    \del{x_p} {
                        \sum_{\substack{i=1\\i\neq p}}^n x_i A_{ip} x_p
                    }
                    =
                    \sum_{\substack{i=1\\i\neq p}}^n { x_i A_{ip} }
                \end{equation*}
                All other terms vanish if $j \neq p$.

                Then, if $i = p$ and as $j$ runs from $1$ to $n$ skipping $p$,
                we have these terms:
                \begin{equation*}
                    \del{x_p} {
                        x_p \sum_{\substack{j=1\\j\neq p}}^n { A_{pj} x_j }
                    }
                    =
                    \sum_{\substack{j=1\\j\neq p}}^n { A_{pj} x_j }
                \end{equation*}
                All other terms vanish if $i \neq p$.

                But wait! If we look at the ``missing'' terms from each of the
                two sums we have computed, we notice that they are precisely
                $A_{pp} x_p$, of which we have two thanks to the
                differentiation of the quadratic term.

                Consequently, we can write the $p$th partial derivative like
                this.
                \begin{equation*}
                    \del{x_p} {\left(
                        \sum_{i,k=1}^n A_{ik}x_ix_k + \sum_{i=1}^n b_i x_i
                    \right)}
                    =
                    \sum_{i=1}^n { (A_{ip} + A_{pi}) x_i }
                    +
                    b_p
                \end{equation*}

                Since we know how to find the $p$th partial derivative for any
                $p$, to compute the Jacobian (the overall derivative of $f$),
                we just run $p$ from $1$ to $n$ and collect the partial
                derivatives in a matrix.
        \end{enumerate}
\end{enumerate}

\end{document}
