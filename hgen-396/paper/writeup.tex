\documentclass[12pt]{article}

\usepackage{hyperref}
\usepackage{float, graphicx}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath, amssymb}

\begin{document}

\begin{titlepage}
    \begin{center}
        ~\\[2.0cm]

        \textsc{
            \LARGE McGill University
        }\\[1.5cm]

        \textsc{
            \Large Undergraduate Research Course in Human Genetics (HGEN 396)
        }\\[1.0cm]

        {
            \huge \bfseries Difference Map Optimizer \\
            a new method for the unconstrained global optimization of
            multivariate scalar functions
        }\\[2.0cm]

        \emph{By}\\
        Jacob Thomas Errington (260636023)\\[1.0cm]

        \emph{Presented to}\\
        Dr. Simon Gravel

        \vfill

        \today
    \end{center}
\end{titlepage}

\begin{abstract}

Global optimizers are valuable tools in many contexts:
whereas some intractable problems are frequently approximated, as exact
solutions may be too costly to compute, certain problems have no analytic
solutions.
We propose a deterministic metaheuristic for the global optimization
problem, using an iteration scheme based on local searches combined with an
internal estimate of the objective function's global minimum.
Our method is empirically compared to existing state-of-the-art methods on
a handful of difficult functions and its perceived strengths and weaknesses
are outlined.
We conclude that although our method is somewhat comparable in effectiveness to
the state-of-the-art as an out-of-the-box tool requiring practically no
tweaking, contemporary methods that are first adjusted by
the practitioner to suit the problem at hand outperform our method.

\end{abstract}

\section{Introduction}

An \emph{optimization problem} is one in which we seek to identify a solution
that is better than other candidate solutions with respect to some measure
of quality.
In a minimization problem, the optimal solution admits the smallest
such measure, whereas in a maximization problem, it admits the greatest such
measure.
This measure of quality is formally called the \emph{objective function} of the
optimization problem, and the value it assigns to each candidate solution is
variously called the \emph{score}, the \emph{energy}, or the \emph{fitness},
depending on the nature of problem.
A \emph{local} optimization problem seeks to find an optimal solution within
some subset of the full range of candidate solutions, whereas a \emph{global}
optimization problem seeks to identify the best solution across the full set of
candidates.

The global optimization problem is stated formally as
$$
\min_{x \in \mathbb{R}^n} f(x)
$$
where $f : \mathbb{R}^n \to \mathbb{R}$,
\emph{i.e.} we wish to find $x^* \in \mathbb{R}^n$
such that $f(x^*) \leq f(x)$ for all $x \in \mathbb{R}^n$.

\emph{Heuristics} are optimization procedures that use problem-specific
information to make and refine guesses about the location of the global
minimum.  They tend to be fairly greedy in their search patterns, and thus can
become trapped in a local minimum when faced with a more general problem.
On the other hand, \emph{metaheuristics} are general optimization procedures
that make very few assumptions about the objective function; a metaheuristic
treats the objective function as a black box and has little additional
knowledge of the problem.

Metaheuristics can avoid becoming trapped in local minima by occasionally
searching in directions that do not appear immediately promising, in the hopes
that this may eventually lead to a better overall solution.
Generally, a metaheuristic does not have very strong guarantees as to the true
optimality of its final discovered solution. In many applications, however, the
solutions found by a metaheuristic are acceptably close to the optimum.
Metaheuristics are especially useful due to their robustness in problems where
the function to optimize is noisy, discrete, or discontinuous.

\emph{Simulated Annealing}\cite{kirkpatrick1983} is perhaps one of the most
well-known and most extensively studied metaheuristics.
The intuition for it comes from the \emph{annealing} procedure in
metallurgy, in which a metal is brought to a high temperature before being
gradually cooled.
The greater energy of the high-temperature particles allows for many
potentially unstable configurations to be tried, in the hopes that as the
metal is slowly cooled, energy-minimizing arrangements will be found.
Thus, the annealing's \emph{cooling schedule} is important in avoiding
local minima, as a slower one would allow the particles more time to
stochastically try many different arrangements, whereas a faster one would
offer them fewer such chances.

A higher temperature value results in more liberal acceptance of new,
higher-energy coordinates.
Hence the search space is broadly explored while the temperature is high.
As the temperature decreases, the acceptance of new coordinates becomes
more selective; at a temperature of zero, only coordinates strictly better
than the current position will be accepted.

The Simulated Annealing algorithm thus consists of two essential steps at each
iteration:
\begin{enumerate}
    \item random perturbation of the coordinates;
    \item accept or reject the new coordinates based on the current
        temperature.
\end{enumerate}

\emph{Basin Hopping}\cite{wales1997} is a popular variation on Simulated
Annealing.
Simply put, a local minimization is performed prior to the accept-or-reject
step, and the accept-or-reject is performed based on the results of that
local search.
Of course, this method is available only to functions in which local
searches are both possible and acceptably effective.
Basin Hopping is thus less robust, but seems to provide better performance
on smooth functions with many local minima.

\emph{Differential Evolution}\cite{storn1997} is a metaheuristic based on
evolutionary processes.
In genetics, \emph{chromosomes} are the basic bundles of information.
The \emph{crossing over} (or \emph{recombination}) of two chromosomes produces
new chromosomes by stochastically exchanging information between them.
\emph{Selection} is the process that removes individuals with low
\emph{fitness} from the population.

In the context of optimization, the chromosomes are candidate solution
vectors, and the exchanged pieces of information roughly correspond to
entries of those vectors. As for selection, the fitness of a candidate solution
is simply given by the objective function.

Each iteration of Differential Evolution traverses the \emph{population} of
candidate solutions and randomly updates coordinates of the candidate vector
according to the \emph{crossover probability}. The strength of the crossover is
given by another parameter called the \emph{differential weight}. The
optimization is typically made to terminate after some number of iterations or
when other problem-specific criteria are satisfied.

Three control parameters given above -- the population size, the crossover
probability, and the differential weight -- greatly affect the effectiveness of
Differential Evolution and must be chosen by the practitioner\cite{storn1997}.
In fact, the choice of these parameters has been the topic of
meta-optimization\cite{pedersen2010}, in which one optimizer is used to
determine the best selection of configuration papmeters for another optimizer.

The \emph{Simplex Method}\cite{nelder1965}, frequently called the
\emph{Nelder-Mead} method after its creators (and to avoid confusion with a
similarly named method for linear programming), is a much older
minimization strategy. Although originally conceived as a local
minimization technique, it can be applied to global minimization as well.

If the dimension of the search space is $n$, then $n+1$ points are first
chosen to form a simplex in the space. The intuition for this method is to
let the simplex tumble downhill towards a local minimum, expanding and
contracting as twists and turns are encountered in the function. In
particular, on each iteration, the vertex with the worst score (highest
function value) is replaced by reflecting it through the centroid of the
simplex. The simplex is expanded or contracted according to the quality of the
reflection.

Again, a number of parameters are again to be chosen by the practitioner,
namely the ratios to use when reflecting, expanding, and contracting the
simplex.


\section{Objective}

In the above survey of existing methods, each optimization technique
requires a number of parameters that must be chosen \emph{a priori}. On the
one hand, these methods are very general thanks to the parameters that can
be tweaked to adapt them to many different problems. On the other hand,
this can make them difficult to use when it is not clear how the parameters
should be chosen.

With that in mind, we wish to design an optimization technique
requiring as little configuration as possible; our method should work
\emph{out-of-the-box} on a variety of problems. Furthermore, we desire
that our method outperform Simulated Annealing.

The rationale for singling out Simulated Annealing is that it is perhaps the
most extensively studied metaheuristic, and is perhaps the most general, having
been greatly customized in specific applications. Equally, we would prefer our
method to have a certain potential to be specialized further.


\section{Methods}

\subsection{Difference Map Optimizer}
The method we propose is based on the \emph{Difference Map}\cite{elser2007}, a
constraint satisfaction algorithm.

The Difference Map solves problems requiring the simultaneous satisfaction of
two constraints by iteratively projecting the iterate onto sets identified with
the constraints.
This makes the Difference Map an \emph{iterative projection scheme}.

Naive iterative projection schemes tend to become trapped in local minima, but
the Difference Map can avoid this precisely by using the difference of the
projections onto the two constraints to be satisfied.
Specifically, this keeps the iterate distinct from the projections,
reducing the greediness of the method and improving its overall ability to
identify the intersection of the constraints.
In the case of global optimization, our intuition is to think
of the function to optimize as a constraint to be satisfied, and to consider
local minimizations as a kind of projection.

Our global optimization method, called the \emph{Difference Map Optimizer}
(DMO), is an iterative metaheuristic that uses local searches on each
iteration to gradually build up a landscape of the search space.
Rather than have the iterate directly follow the local search, the results of
the local searches are kept distinct from the iterate; this acts to avoid
stagnation in local minima as in the Difference Map.

As a mechanism for adapting to different functions, DMO maintains an internal
estimate of the global minimum, called the \emph{target} and denoted $t$,
which affects the sizes of the steps taken by the iterate as it explores the
search space.

First, two starting points $x_0$ and $x_1$ are selected from the search
space.
A local minimization is performed on $x_0$ to find $x_0^*$ and the
initial value for the target is computed as
$$t := f(x_0^*) - s (f(x_0) - f(x_0^*))$$
where $s$ is a positive scaling factor that controls the overall greediness of
the algorithm.
The point $x_1$ is taken as the initial position of the iterate. Our
experiments showed that $x_0$ and $x_1$ should be chosen to be close to
each other, yet ideally in different basins of attraction.
Formally, a basin of attraction is a region in which local minimization at
each point leads to the same local minimum, and the point giving the local
minimum is called a \emph{minimizer}.
The points $x_0^*$ and $x_0$ are then added to a running list $M$ of
local minimizers so far.

On each iteration starting from the position of the iterate $x_i$, a local
minimizer $x_i^*$ is found by traditional local optimization techniques.
If $f(x_i^*)$ is less than the current target, then the target is
recalculated as before, but using $f(x_i^*)$ and $\min_{x \in M} f(x)$.

Next, we devise a step-taking procedure that corresponds to a second kind
of projection. Since we already have the list $M$ representing the results of
the first type of projection, we wish to combine somehow these results to
produce a new position for the iterate. Borrowing the ideas of the Difference
Map, we choose to take a difference of the element in $M$ nearest to the
current local minimizer to determine the direction of the step. This step is
then scaled such that it acts as a projection onto the target. Finally, the
calculated step is added to the iterate.

The intuition for such a procedure derives directly from the Difference Map;
by applying a difference of previously discovered local minima to the iterate,
we keep the minima and the iterate distinct and avoid stagnation.

Figure \ref{fig:iteration} provides a visualization of the step-taking
procedure.

Precisely, we select the point $x_{near} \in M$ having the least distance to
$x_i^*$.
From $x_{near}$, the step $\Delta x$ to take is calculated as
\begin{equation*}
    \Delta x_i =
        \frac{t - f(x_i^*)}{f(x_i^*) - f(x_{near})} (x_i^* - x_{near})
    \label{eqn:dx}
\end{equation*}

The next position of the iterate is obtained by applying the step
$\Delta x_i$.
$$
    x_{i+1} = x_i + \Delta x_i
$$

Note that if the quantity $f(x_i^*) - f(x_{near})$ is close to zero,
\emph{i.e.} $|f(x_i^*) - f(x_{near})| < \epsilon$ where $\epsilon$ is a
predetermined tolerance for numerically distinguishing reals, then that
quantity should be replaced with $\epsilon$.  Under these circumstances,
which arise when the local landscape of the function is flat, the iterate is
to be moved infinitely far away.  Of course this is not possible, so a
numerical approximation is chosen instead.

Finally, the point $x_i^*$ is added to the set of discovered minimizers
$M$, and the target value $t$ is updated.

Many target update strategies are possible, but our experiments showed that
merely recalculating the target, as above, every $r$ iterations using the two
smallest values $f(x)$ for $x \in M$ produced acceptable estimates of the
global minimum.

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=0.8\textwidth]{../figures/iterating.pdf}
        \caption{
            An example iteration of the Difference Map Optimizer.
            $x_i$ is the current position of the iterate, and $x_i^*$ is the
            local minimizer in the neighborhood of $x_i$. $x_{near}$ is
            previously discovered local minimizer closest to $x_i^*$.
            The line passing through $x_{near}$ and $x_i^*$ intersects the
            line of the current target; the effect of equation \ref{eqn:dx}
            is to project $x_i^*$ onto the target. The displacement found by
            that projection is then applied to the current iterate $x_i$ to
            find the next iterate, $x_{i+1}$.
        }
        \label{fig:iteration}
    \end{center}
\end{figure}


\subsection{Benchmarking}

Wolfram \emph{Mathematica} provides implementations of several of the
optimization methods presented earlier.
In particular, Simulated Annealing, Differential Evolution, and Nelder-Mead
are implemented.
For that reason, we choose \emph{Mathematica} as our primary environment for
testing DMO.
All tests are conducted with version $10.0$ of \emph{Mathematica} on an
Apple iMac (late 2013) running Ubuntu, with a $3.1$~GHz Intel i7 processor
and $16$~GB of $1600$~MHz DDR3 RAM.
Our reference implementation of DMO can be found online at
\url{https://github.com/djeik/dm_optimizer}.

First, since our primary goal is to design an out-of-the-box algorithm,
we compare DMO to each of the \emph{Mathematica} optimizers with as little
tweaking as possible.

Second, a minor tweak to the \emph{Mathematica} optimizers is applied. This
will allow a comparison between DMO, which requires no such tweaking,
and solvers that are effectively being advantaged by this extra information
concerning the problem space.

The tweak in question is essentially a \emph{bracketing} of the search
space. In some applications it is possible to bracket the optimum, but in
others, it is not. Hence, we believe that it is worthwhile to explore both
these avenues. It is worth noting, however, that applying this bracketing to
the \emph{Mathematica} optimizers does not prevent the search from leaving the
supplied region; it simply provides a space in which the optimizer can sample
initial points and perhaps choose some kind of scale for the problem.

In order to compare the solvers in the fairest way possible, each solver is
run such that the average number of function evaluations per run is
approximately $20~000$.

As the connection between the maximum number of allowed interations and the
number of function evaluations is rather unclear for the \emph{Mathematica}
methods, a particular proportion was used to estimate how many iterations
each solver should be run for in order to obtain the desired number of
function evaluations.
Specifically, each solver is run for some small number of iterations,
\emph{e.g.} $5$, and the number of function evaluations is counted.
A ratio of function evaluations per iteration is computed and scaled to
match the desired $20~000$ iterations.

The reason we choose to average over a small number of iterations rather
than a larger one is due to an oddity in \emph{Mathematica}.
In the region of small iteration counts, the number of function evaluations
appears to scale roughly linearly, but in higher ranges this appears to be
no longer true.
For instance, some of our tests showed that Simulated Annealing would seem
to stall around $750$ function evaluations, no matter how high the number
of iterations was set.

Stalls in the \emph{Mathematica} optimizers are dealt with by a random
restart process: if the solver generates an insufficient number of function
evaluations, then the solver is restarted with a different random seed.
It should be noted that a new starting point for the solver is not selected, as
this could improve the odds of the solver landing in the global minimum at the
first iteration, which would unfairly advantage the randomly restarting
solvers. Thus, by changing only the seed for the random number generator,
it is only the trajectory of the solver that is affected.

\pagebreak

\section{Results}
\begin{figure}[H]
    \begin{center}
        {\large {\bfseries Solutions found per solver per function}\\
        for unbracketed \emph{Mathematica} solvers}

        \includegraphics[width=0.45\textwidth]{../figures/ackley-fun-bad.pdf}
        \includegraphics[width=0.45\textwidth]{../figures/griewank-fun-bad.pdf}
        \includegraphics[width=0.45\textwidth]{../figures/rosenbrock-fun-bad.pdf}
        \includegraphics[width=0.45\textwidth]{../figures/f1-fun-bad.pdf}
        \includegraphics[width=0.45\textwidth]{../figures/h4-fun-bad.pdf}

        \caption{
            A distribution chart of the global minimum found for each
            solver, for each tested objective function. For each solver,
            $25$~runs were performed, with the iteration count for each
            solver adjusted to perform a total of approximately $20~000$
            function evaluations. The \emph{Mathematica} solvers that
            generated this dataset were \emph{not} given bracketing
            information concerning the objective function.
        }
        \label{fig:bad-data}
    \end{center}
\end{figure}

From figure \ref{fig:bad-data}, we can see that DMO performs comparably to
Simulated Annealing in the Ackley, Griewank, F1 functions, but is
outperformed in the Rosenbrock and H4 functions.

\begin{figure}[H]
    \begin{center}
        {\large {\bfseries Exploration patterns of \emph{Mathematica}'s solvers}\\
        for both unbracketed and bracketed configurations}
        \includegraphics[width=\textwidth]{../figures/mma-compare-brackets.pdf}
        \caption{The exploration patterns of the built-in
        \emph{Mathematica} optimizers. The explorations on the top row are
            generated by using the default sampling range in the search
            space, $-1 < x_i < 1$ for $1 \leq i \leq n$ where $n$ is the
            dimension of the search space. On the bottom row, the sampling
            range is much larger. The objective function used in the above
            is the \emph{Griewank} function, for which the suggested
            evaluation range is $-600 < x_i < 600$ for each
            $x_i$\cite{griewank1981}. The change in scale between the plots
            in the top and bottom rows is large. % TODO this is lame.
        }
        \label{fig:exploration}
    \end{center}
\end{figure}

Figure \ref{fig:exploration} demonstrates the crucial difference made by
endowing the solvers with the bracketing information concerning the search
space. The search space is explored far more effectively, as one would
expect. We suspect that this information is used by Simulated Annealing to
determine by how much it should perturb its coordinate vector. Likewise, we
believe that Differential Evolution can create its population of candidate
vectors in a more principled and spread out manner thanks to this
additional information.

\begin{figure}[H]
    \begin{center}
        {\large {\bfseries Comparing exploration patterns of DMO and Simulated Annealing}\\
        with both bracketed and unbracketed Simulated Annealing runs}
        \includegraphics[width=0.8\textwidth]{../figures/ackley-dm-sa-bracketing-compare.pdf}

        \caption{
            A comparison of the exploration patterns of DMO and Simulated
            Annealing on the Ackley function. Without bracketing
            information, Simulated Annealing (orange) performs very badly,
            and hardly explores the search space. Given this additional
            bracketing information, its exploration (blue) is similar to
            DMO's (green).
        }
        \label{fig:dm-sa-bracket-exploration}
    \end{center}
\end{figure}


With this additional information, the mathematica solvers do outperform DMO
in all cases, as seen in figure \ref{fig:good-data}.

\begin{figure}[H]
    \begin{center}
        {\large {\bfseries Solutions found per solver per function}\\
        for bracketed \emph{Mathematica} solvers}

        \includegraphics[width=0.45\textwidth]{../figures/ackley-good.pdf}
        \includegraphics[width=0.45\textwidth]{../figures/griewank-good.pdf}
        \includegraphics[width=0.45\textwidth]{../figures/rosenbrock-good.pdf}
        \includegraphics[width=0.45\textwidth]{../figures/f1-good.pdf}
        \includegraphics[width=0.45\textwidth]{../figures/h4-good.pdf}

        \caption{
            A distribution chart of the global minimum found for each
            solver, for each tested objective function. For each solver,
            $25$~runs were performed, with the iteration count for each
            solver adjusted to perform a total of approximately $20~000$
            function evaluations. The \emph{Mathematica} solvers that
            generated this dataset were given bracketing information
            concerning the objective function.
        }
        \label{fig:good-data}
    \end{center}
\end{figure}

\pagebreak

\section{Conclusion}

We have presented Difference Map Optimizer, a metaheuristic for the global
optimization of scalar functions of several variables.
The intuition for it comes from the Difference Map, a constraint satisfaction
algorithm of surprising efficiency and simplicity.

The essential property of DMO is that it requires hardly no configuration, and
is able to traverse the search space in a principled manner without becoming
trapped in local minima.
Furthermore, unlike contemporary methods that seek to emulate stochastic
processes as effective optimization algorithms, DMO is deterministic.

In problems for which information concerning the search space is very limited,
DMO may show a lot of promise.
However, in problems where information about the search space is available,
other methods are at this time still more appropriate.

Overall, the Difference Map Optimizer is interesting due to these perceived
strengths and its sheer novelty as an optimization technique. Although it is
not presently ready for use in production environments, some different
approaches to improving it are clear.


\subsection{Future directions}

There is still substantial room for improvement in DMO.
For instance, the outliers in figures \ref{fig:good-data} and
\ref{fig:bad-data} in the Rosenbrock function are perhaps the result of DMO
having no intrinsic way to ensure that its search converges, or stays within
reasonable limits.
Indeed, that is in some sense a strength of DMO: it has no notion of what is
reasonable beyond its internal estimate of the global minimum.
Moreover, whereas Simulated Annealing progressively becomes more
conservative in its choices as time goes by, DMO does not even keep track of
how much time has elapsed beyond how many local minima have been discovered
thus far.

Although the step-taking strategy we use comes from our intuition based on
the Difference Map, there may be alternative strategies that are more
effective. Specifically, the strategy that we employ uses a single
previously discovered minimum, but a strategy using more than one previously
discovered local minimum, for instance, may offer improved results.

Another avenue for potential improvement concerns the target and the methods
used for calculating it. In our current scheme, it suffices that just two
minima have similar values for the target to become quite conservative. The
effect of a conservative target is to shrink the steps taken by the iterate,
which can considerably slow down the search. Hence, alternative target
calculation strategies using more advanced sampling techniques may improve
the effectiveness and self-regulating nature of the search.

\pagebreak

\bibliographystyle{plain}
\bibliography{dm}

\end{document}
